{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点：\n",
    "\n",
    "1. LSTM 层的输入（input）格式 ---> (batch_size, sequence_length, number_features)\n",
    "- 参数讲解： \n",
    "- batch_size : 每批多少个序列\n",
    "- sequence_length : 序列长度（步长）\n",
    "- number_features : 特征个数\n",
    "\n",
    "2. LSTM 层的输出（output）格式 --> (batch_size, sequence_length, hidden_size)\n",
    "- 参数讲解：\n",
    "- batch_size : 每批多少个序列\n",
    "- sequence_length : 序列长度\n",
    "- hidden_size : 隐藏层节点node个数\n",
    "\n",
    "3. Linear 层的输入（input）格式 --> (batches, n_hidden)\n",
    "- 参数讲解：\n",
    "- batches : 有多少个batch_size\n",
    "- n_hidden : 隐藏层节点node个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟演示\n",
    "\n",
    "- input_size = 1 # 特征个数\n",
    "- hidden_size = 100 # LSTM层中100个隐层结点\n",
    "- n_layers = 2 # 如果是堆叠LSTM，表示为2层lstm\n",
    "- output_size = 1 # LSTM的输出大小\n",
    "\n",
    "- lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True) # LSTM层\n",
    "- linear = nn.Linear(hidden_size, 1) # 全连接层\n",
    "\n",
    "- x = get_batches(data) # 构造新的数据集，输入格式：(batch_size, seq_len, num_features)\n",
    "- x, h_s = lstm(x, hidden_size) # LSTM output : (batch_size, seq_len, hidden_size)\n",
    "- x = x.reshape(-1, hidden_size) # Linear in : (batch_size * seq_len, hidden_size)\n",
    "- x = linear(x) # linear out : (batch_size * seq_len, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例：文本预测（本质上是分类问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "class lstm_model(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, num_layers, dropout=0.5):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.vocab = vocab # 字符数据集\n",
    "        # 索引 : 字符\n",
    "        self.int_char = {i : char for i, char in enumerate(vocab)} # 另一种写法：self.int_char = dict(enumerate(vocab))\n",
    "        # 字符 : 索引\n",
    "        self.char_int = {char : i for i, char in self.int_char.items()}\n",
    "        # 对字符进行one-hot encoding\n",
    "        self.encoder = OneHotEncoder(sparse=False).fit(vocab.reshape(-1, 1)) # 这里需要对vocab进行shape转换\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # lstm层\n",
    "        self.lstm = nn.LSTM(len(vocab), hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # 全连接层\n",
    "        self.linear = nn.Linear(hidden_size, len(vocab)) # 这里的输出shape是每个字符的得分\n",
    "\n",
    "    def forward(self, sequence, hs=None):\n",
    "        out, hs = self.lstm(sequence, hs) # lstm的输出格式：（batch_size, sequence_length, hidden_size）\n",
    "        out = out.reshape(-1, self.hidden_size) # 这里需要将out转换为linear的输入格式，即(batch_size*sequence_length, hidden_size)\n",
    "        output = self.linear(out) # linear的输出格式：((batch_size*sequence_length, vocab_size)\n",
    "        return output, hs\n",
    "\n",
    "    def onehot_encode(self, data):\n",
    "        return self.encoder.transform(data)\n",
    "\n",
    "    def onehot_decode(self, data):\n",
    "        return self.encoder.inverse_transform(data)\n",
    "\n",
    "    def label_encode(self, data):\n",
    "        return np.array([self.char_int[ch] for ch in data])\n",
    "\n",
    "    def label_decode(self, data):\n",
    "        return np.array([self.int_char[ch] for ch in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义构建新数据集的批处理方法\n",
    "def get_batches(data, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    参数\n",
    "    -------------\n",
    "    data : 源数据，输入格式（num_samples, num_features）\n",
    "    batch_size : batch的大小\n",
    "    seq_len : 序列的长度（跨度）\n",
    "\n",
    "    return\n",
    "    -------------\n",
    "    新的数据集，格式：（batch_size, seq_len, num_features）\n",
    "    \"\"\"\n",
    "    num_features = data.shape[1] # 数据的列数，即特征数(本案例83个不同的字符)\n",
    "    #print(\"num_features : \", num_features)\n",
    "    num_chars = batch_size * seq_len # 一个batch_size的字符数量（本案例12800个字符，128 * 100 = 12800）\n",
    "    #print(\"num_chars : \", num_chars)\n",
    "    num_batches = int(np.floor(len(data) / num_chars)) # 计算出有多少个batches（本案例中文本数据最多有124个batches）\n",
    "    #print(\"num_batches : \", num_batches)\n",
    "    need_chars = num_batches * num_chars # 根据batch_size 和 batches 计算出所需的总字符数量（本案例共需1587200个字符）\n",
    "    #print(\"need_chars : \", need_chars)\n",
    "    targets = np.append(data[1:], data[0]).reshape(data.shape) # 标签数据，注意：标签数据是往后全部挪一位\n",
    "    #print(\"targets shape \", targets.shape) # [1588179, 83]\n",
    "    #print(\"data.shape : \", data.shape) # [1588179, 83]\n",
    "\n",
    "    inputs = data[:need_chars] # 从原始数据data中截取所需的字符数量need_words\n",
    "    targets = targets[:need_chars] #从原始标签targets中截取所需的字符数量need_words\n",
    "\n",
    "    #print(\"inputs.shape : \", inputs.shape) # (1587200, 83)\n",
    "    #print(\"targets.shape : \", targets.shape) # (1587200, 83)\n",
    "\n",
    "    # shape转换\n",
    "    inputs = inputs.reshape(batch_size, -1, num_features)\n",
    "    targets = targets.reshape(batch_size, -1, num_features)\n",
    "    #print(\"inputs reshape : \", inputs.shape) # (128, 12400, 83)\n",
    "    #print(\"targets reshape : \", targets.shape)\n",
    "\n",
    "    # 构建新的数据集\n",
    "    for i in range(0, inputs.shape[1], seq_len):\n",
    "        x = inputs[:, i : i+seq_len]\n",
    "        y = targets[:, i : i+seq_len]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, data, batch_size, seq_len, epochs, lr=0.01, valid=None):\n",
    "    '''\n",
    "    参数说明\n",
    "    -----------\n",
    "    model : 定义的字符级网络模型\n",
    "    data  : 文本数据\n",
    "    batch_size : 一个batch多少个数据\n",
    "    seq_len : 序列长度（步长）\n",
    "    epochs : 训练循环次数\n",
    "    lr : 学习率\n",
    "    valid : 验证数据\n",
    "    '''\n",
    "    # 是否有cuda\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # 部署模型到device\n",
    "    model = model.to(device)\n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 判断是否有valid数据（即是否边训练边验证）\n",
    "    if valid is not None:\n",
    "        data = model.onehot_encode(data.reshape(-1, 1))\n",
    "        valid = model.onehot_encode(valid.reshape(-1, 1))\n",
    "    else:\n",
    "        data = model.onehot_encode(data.reshape(-1, 1))\n",
    "    # 保存损失值\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # 循环训练（验证）\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        hs = None # hs 等于 hidden_size,隐藏层结点\n",
    "        train_ls = 0.0\n",
    "        val_ls = 0.0\n",
    "        for x, y in get_batches(data, batch_size, seq_len):\n",
    "            #print(\"y.shape_1 : \", y.shape) # (128, 100, 83)\n",
    "            # 每一轮循环，生成一批 数据+标签（data+target）\n",
    "            optimizer.zero_grad() # 梯度置零\n",
    "            x = torch.tensor(x).float().to(device) # 类型转换\n",
    "            #print(\"x shape : \", x.shape) # torch.Size([128, 100, 83])\n",
    "            # 模型训练\n",
    "            out, hs = model(x, hs) # 模型输出shape : （batch_size, sequence_length, hidden_size）\n",
    "            hs = ([h.data for h in hs]) # 读取每一个hidden_size的结点\n",
    "            # 对targets的one-hot encoding进行逆向转换\n",
    "            y = y.reshape(-1, len(model.vocab))\n",
    "            #print(\"y.shape_2 : \", y.shape) # (12800, 83)\n",
    "            y = model.onehot_decode(y)\n",
    "            #print(\"y.shape_3 : \", y.shape) # (12800, 1)\n",
    "            # 对y进行label encoding\n",
    "            y = model.label_encode(y.squeeze())\n",
    "            #print(\"y.shape_4 : \", y.shape) # (12800,)\n",
    "            # 类型转换\n",
    "            y = torch.from_numpy(y).long().to(device)\n",
    "            #print(\"y.shape_5 : \", y.shape) # torch.Size([12800])\n",
    "            # 计算损失函数\n",
    "            loss = criterion(out, y.squeeze())\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            # 累计训练损失\n",
    "            train_ls += loss.item()\n",
    "\n",
    "        if valid is not None:\n",
    "            # 开始验证\n",
    "            model.eval()\n",
    "            hs = None\n",
    "            with torch.no_grad():\n",
    "                for x, y in get_batches(valid, batch_size, seq_len):\n",
    "                    x = torch.tensor(x).float().to(device)\n",
    "                    out, hs = model(x, hs) # 预测输出\n",
    "                    hs = ([h.data for h in hs])\n",
    "\n",
    "                    y = y.reshape(-1, len(model.vocab))\n",
    "                    y = model.onehot_decode(y)\n",
    "                    y = model.label_encode(y.squeeze())\n",
    "                    y = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "                    loss = criterion(out, y.squeeze())\n",
    "                    val_ls += loss.item()\n",
    "                val_loss.append(np.mean(val_ls)) # 求出每一轮的损失均值，并累计\n",
    "\n",
    "            train_loss.append(np.mean(train_ls)) # 求出每一轮的损失均值，并累计\n",
    "\n",
    "        print(f'--------------Epochs{epochs} | {epoch}---------------')\n",
    "        print(f'Train Loss : {train_loss[-1]}') # 这里-1为最后添加进去的loss值，即本轮batch的loss\n",
    "        if val_loss:\n",
    "            print(f'Val Loss : {val_loss[-1]}')\n",
    "\n",
    "    # 绘制loss曲线\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "with open(\"anna.txt\") as data:\n",
    "    text = data.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示前100个字符\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-',\n",
       "       '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':',\n",
       "       ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
       "       'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
       "       'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
       "       'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
       "       'v', 'w', 'x', 'y', 'z'], dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选出文本数据中不同的字符\n",
    "vocab = np.array(sorted(set(text)))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符的数量\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分text为train和val\n",
    "# 假设 val 占比20%\n",
    "val_len = int(np.floor(0.2 * len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1588179,)\n",
      "(397044,)\n"
     ]
    }
   ],
   "source": [
    "# train 和 val\n",
    "trainset = np.array(list(text[:-val_len]))\n",
    "validset = np.array(list(text[-val_len:]))\n",
    "\n",
    "print(trainset.shape)\n",
    "print(validset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=512, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义超参数\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "seq_len = 100\n",
    "epochs = 20\n",
    "lr = 0.01\n",
    "\n",
    "# 创建模型对象\n",
    "model = lstm_model(vocab, hidden_size, num_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.73779320716858\n",
      "--------------Epochs20 | 0---------------\n",
      "Train Loss : 390.5807454586029\n",
      "Val Loss : 96.73779320716858\n"
     ]
    }
   ],
   "source": [
    "train(model, trainset, batch_size, seq_len, epochs, lr=lr, valid=validset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, top_k = None, hidden_size = None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        char = np.array([char]) # 转换为array\n",
    "        char = char.reshape(-1, 1) # shape转换\n",
    "        char_encoding = model.onehot_encode(char) # encoding\n",
    "        char_encoding = char_encoding.reshape(1, 1, -1) # (batch_size, seq_len, num_features)\n",
    "        char_tensor = torch.tensor(char_encoding, dtype=torch.float32) # 类型转换\n",
    "        char_tensor = char_tensor.to(device) # 部署到device上\n",
    "        \n",
    "        out, hidden_size = model(char_tensor, hidden_size) # 模型预测\n",
    "\n",
    "        probs = F.softmax(out, dim=1).squeeze() # torch.Size([1, 83]) --> torch.Size([83])\n",
    "        #probs = F.softmax(out, dim=1).data # 另一种写法，结果一致\n",
    "        \n",
    "        \n",
    "        if top_k is None:\n",
    "            indices = np.arange(vocab_size)\n",
    "        else:\n",
    "            probs, indices = probs.topk(top_k) # 选取概率最大的前top_k个\n",
    "            indices = indices.cpu().numpy()\n",
    "        \n",
    "        probs = probs.cpu().numpy()\n",
    "        \n",
    "        char_index = np.random.choice(indices, p = probs / probs.sum()) # 随机选取一个索引\n",
    "        char = model.int_char[char_index] # 获取索引对应的字符\n",
    "        \n",
    "    return char, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取一个样本\n",
    "def sample(model, length, top_k = None, sentence=\"every unhappy family \"):\n",
    "    hidden_size = None # 初始化\n",
    "    new_sentence = [char for char in sentence] # 初始化\n",
    "    for i in range(length):\n",
    "        next_char, hidden_size = predict(model, new_sentence[-1], top_k = top_k, hidden_size = hidden_size) # 预测下一个字符\n",
    "        new_sentence.append(next_char)\n",
    "\n",
    "    return ''.join(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = sample(model, 2000, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'every unhappy family cheed ale tall.\\nAnd he welle as than stitt hou the hart and sooker..\".\\n\\n\"Yons the with and ale\\nfise ale, and seet ont his a coud. When the wast and wered\\nand alraly that sert, evere the shull,\\nand see and her the hims to the seest to me the storte the ther hout to ware the cortitsing whout hir\\nstour the her the well\\nats her, bet te storted has, and whel so te tale thime and the his the stert oft ont. He a cert him,\" that hent to hid to to maller and\\ntoone to the segt the cand that sart to with, at to to wame they was wishting\\nhe wamite the tong the come were shey him, and he was soud as to mort the the cowed and the some would. Whing,\" the he with that her.\"\"\\n\\n\"Ne a come hens a ducling her and ather were, the harken the has of\\nthere was the sers ot this thind weringes hered hams, the tooded was. White, whan strerend aly the\\npronened her thil ale that to\\nwenl, therit and that asding, whing, and seed steid the hert. He the saserter,\" saste a derederes, worting and she serted onte worte the\\nsellill to the her his that wam the was hou tuln.\\n\\n\"What soring a cored oner one,\" had the starly his wam and and shored, there the stele angar hel and the cout woting would was at and the with her the sert to was to hame him, and the tory and, whound hor to\\ntoes a che his samed of in shery her the same an and ald hus aghor, that a she steed and sersion and, and at he stele she his and te sterent him to miss the cowst wont of to musse was the cunded.\\n\\n\"And, and to seen as in\\nso wear. Whe stellicg wotion. Teed and anle,\\nand wery the was the salks, he stere the sherse sorill the\\nher the toned shen toed,\"\\nAs him all ats, him to ware all. Whime the wound thint hin wish had sorer out of the stale titite to his shink and hild with shey this\\nthe hom and sorlered wat the werlict wham has she serticd, eving, as have at\\nselt,\" nore oute oned they and of he agitered the was as\\nand wan the tone his thind all had\\nto wat wed\\ncousted, and the ham to tuln then wound thind,\\nassed and an helted had c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lstm_model.net\"\n",
    "\n",
    "checkpoint = {\n",
    "    'hidden_size' : model.hidden_size,\n",
    "    'num_layers' : model.num_layers,\n",
    "    'state_dict' : model.state_dict()\n",
    "}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b55140bc9bd5084f21fbaa66b1edd89209bd8dc911d8d1825c542d6d54ebb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
