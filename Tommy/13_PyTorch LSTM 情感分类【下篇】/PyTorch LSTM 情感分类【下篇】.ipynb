{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现流程：\n",
    "\n",
    "#### 1. 读取原始数据集（文本集）\n",
    "\n",
    "#### 2. 文本预处理\n",
    "* **2.1 清理无用的标点符号**\n",
    "* **2.2 根据 换行符 \\n 分割**\n",
    "* **2.3 单词 --> 索引 转换**\n",
    "* **2.4 标签 --> 1， 0 转换**\n",
    "* **2.5 清理文本太短以及过长的样本**\n",
    "* **2.6 将单词映射为整型**\n",
    "* **2.7 设定统一的文本长度，对整个文本数据中的每条评论进行填充或截断**\n",
    "\n",
    "#### 3. 特征工程\n",
    "* **3.1 array --> tensor**\n",
    "* **3.2 将数据集分离成：train, val, test 三部分，比例是： 0.8, 0.1, 0.1**\n",
    "* **3.3 通过DataLoader按批处理数据**\n",
    "\n",
    "#### 4. 定义网络模型结构\n",
    "\n",
    "#### 5. 定义超参数\n",
    "\n",
    "#### 6. 定义训练函数（训练 + 验证）\n",
    "\n",
    "#### 7. 定义测试函数\n",
    "\n",
    "#### 8. 定义预测函数\n",
    "\n",
    "----------------------------\n",
    "\n",
    "* **[B站账号： 唐国梁Tommy]** <https://space.bilibili.com/474347248/channel/index>\n",
    "* **代码+数据集下载，请查看我的B站个人简介**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载文本和标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本数据\n",
    "with open(\"data/reviews.txt\", 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33678267"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) # 共33678267个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text) # 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell h'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10] # 显示前10个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取标签数据\n",
    "with open('data/labels.txt', 'r') as file:\n",
    "    labels = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels) # 共225000个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels) # 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\nn'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10] # 显示前10个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 数据 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标点符号 :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# 2.1 清理无用的标点符号\n",
    "from string import punctuation\n",
    "\n",
    "print(\"标点符号 : \", punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = ''.join([char for char in text if char not in punctuation]) # 遍历文本中每一个字符，跳过标点符合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33351075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text) # 新的文本字符个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 根据 换行符 \\n 分割\n",
    "clean_text = clean_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签 根据 \\n 分割\n",
    "labels = labels.split('\\n')\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative', 'positive', 'negative', 'positive']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 字典： 单词 --> 索引\n",
    "\n",
    "# 获取所有评论中的每个单词\n",
    "words = [word.lower() for sentence in clean_text for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '', 'it', 'ran', 'at']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10] # 显示前10个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_words = list(set(words)) # 筛选出所有评论中不同的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_words.remove('') # 清理空字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(various_words) # 不同的单词个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典，格式： 单词 ： 整数\n",
    "\n",
    "int_word = dict(enumerate(various_words, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'organic',\n",
       " 2: 'tigerland',\n",
       " 3: 'telescope',\n",
       " 4: 'mediocrities',\n",
       " 5: 'proto',\n",
       " 6: 'cutout',\n",
       " 7: 'tomlinson',\n",
       " 8: 'dhawan',\n",
       " 9: 'vegtigris',\n",
       " 10: 'ripner',\n",
       " 11: 'sly',\n",
       " 12: 'putated',\n",
       " 13: 'szwarc',\n",
       " 14: 'soviets',\n",
       " 15: 'brady',\n",
       " 16: 'improbabilities',\n",
       " 17: 'fearlessly',\n",
       " 18: 'cerletti',\n",
       " 19: 'psychiatrically',\n",
       " 20: 'investigates',\n",
       " 21: 'reptilian',\n",
       " 22: 'seawright',\n",
       " 23: 'tirol',\n",
       " 24: 'predicts',\n",
       " 25: 'arnis',\n",
       " 26: 'buttress',\n",
       " 27: 'jordowsky',\n",
       " 28: 'fairbanks',\n",
       " 29: 'everytime',\n",
       " 30: 'kolya',\n",
       " 31: 'francesco',\n",
       " 32: 'wen',\n",
       " 33: 'asset',\n",
       " 34: 'verson',\n",
       " 35: 'whines',\n",
       " 36: 'commishioner',\n",
       " 37: 'angelica',\n",
       " 38: 'teenagers',\n",
       " 39: 'pausing',\n",
       " 40: 'wkrp',\n",
       " 41: 'manpower',\n",
       " 42: 'chromium',\n",
       " 43: 'arkush',\n",
       " 44: 'darkly',\n",
       " 45: 'genes',\n",
       " 46: 'muere',\n",
       " 47: 'youand',\n",
       " 48: 'showcases',\n",
       " 49: 'pickman',\n",
       " 50: 'shumachers',\n",
       " 51: 'corneau',\n",
       " 52: 'enlivening',\n",
       " 53: 'invocus',\n",
       " 54: 'kringle',\n",
       " 55: 'sahi',\n",
       " 56: 'durden',\n",
       " 57: 'supermortalman',\n",
       " 58: 'mewing',\n",
       " 59: 'cowhand',\n",
       " 60: 'fillet',\n",
       " 61: 'grimmest',\n",
       " 62: 'coproduction',\n",
       " 63: 'gelatin',\n",
       " 64: 'retardedness',\n",
       " 65: 'heidijean',\n",
       " 66: 'frustrates',\n",
       " 67: 'talk',\n",
       " 68: 'local',\n",
       " 69: 'oaths',\n",
       " 70: 'shortens',\n",
       " 71: 'unholy',\n",
       " 72: 'travelers',\n",
       " 73: 'entendre',\n",
       " 74: 'petey',\n",
       " 75: 'bamrha',\n",
       " 76: 'unboring',\n",
       " 77: 'heuristic',\n",
       " 78: 'thieriot',\n",
       " 79: 'crockazilla',\n",
       " 80: 'convent',\n",
       " 81: 'overexplanation',\n",
       " 82: 'attachs',\n",
       " 83: 'restaurants',\n",
       " 84: 'monthly',\n",
       " 85: 'solves',\n",
       " 86: 'circuitry',\n",
       " 87: 'sarcastic',\n",
       " 88: 'chuckle',\n",
       " 89: 'larrikin',\n",
       " 90: 'bustle',\n",
       " 91: 'maman',\n",
       " 92: 'dramatisations',\n",
       " 93: 'cherubs',\n",
       " 94: 'crazes',\n",
       " 95: 'tramonti',\n",
       " 96: 'waxed',\n",
       " 97: 'uss',\n",
       " 98: 'silverstonesque',\n",
       " 99: 'malditos',\n",
       " 100: 'camped',\n",
       " 101: 'transmitted',\n",
       " 102: 'enormous',\n",
       " 103: 'cramp',\n",
       " 104: 'reminisces',\n",
       " 105: 'damiano',\n",
       " 106: 'copped',\n",
       " 107: 'bafflement',\n",
       " 108: 'opra',\n",
       " 109: 'shrink',\n",
       " 110: 'kostic',\n",
       " 111: 'caterers',\n",
       " 112: 'droned',\n",
       " 113: 'bhagam',\n",
       " 114: 'langford',\n",
       " 115: 'geisel',\n",
       " 116: 'bilitis',\n",
       " 117: 'cruse',\n",
       " 118: 'proves',\n",
       " 119: 'rekindling',\n",
       " 120: 'unforunatley',\n",
       " 121: 'thoughtful',\n",
       " 122: 'bucket',\n",
       " 123: 'diane',\n",
       " 124: 'grasping',\n",
       " 125: 'gloved',\n",
       " 126: 'superlow',\n",
       " 127: 'remedial',\n",
       " 128: 'piddles',\n",
       " 129: 'invested',\n",
       " 130: 'bypasses',\n",
       " 131: 'proletariat',\n",
       " 132: 'debi',\n",
       " 133: 'mommy',\n",
       " 134: 'swaps',\n",
       " 135: 'chasey',\n",
       " 136: 'animalshas',\n",
       " 137: 'beaute',\n",
       " 138: 'bates',\n",
       " 139: 'poppycock',\n",
       " 140: 'dim',\n",
       " 141: 'crooke',\n",
       " 142: 'testaverdi',\n",
       " 143: 'wheedle',\n",
       " 144: 'slumps',\n",
       " 145: 'sequels',\n",
       " 146: 'cuddlesome',\n",
       " 147: 'newer',\n",
       " 148: 'viewed',\n",
       " 149: 'willaims',\n",
       " 150: 'grandiose',\n",
       " 151: 'polygamy',\n",
       " 152: 'rebounding',\n",
       " 153: 'kidnappedin',\n",
       " 154: 'tomita',\n",
       " 155: 'biros',\n",
       " 156: 'benchley',\n",
       " 157: 'syncretism',\n",
       " 158: 'bullfighter',\n",
       " 159: 'garfiled',\n",
       " 160: 'shouldn',\n",
       " 161: 'fixation',\n",
       " 162: 'elope',\n",
       " 163: 'enveloped',\n",
       " 164: 'renewed',\n",
       " 165: 'carcass',\n",
       " 166: 'draws',\n",
       " 167: 'farlinger',\n",
       " 168: 'debuted',\n",
       " 169: 'charity',\n",
       " 170: 'divx',\n",
       " 171: 'kc',\n",
       " 172: 'novacaine',\n",
       " 173: 'buchman',\n",
       " 174: 'letdown',\n",
       " 175: 'kempo',\n",
       " 176: 'ellery',\n",
       " 177: 'classiest',\n",
       " 178: 'unused',\n",
       " 179: 'foywonder',\n",
       " 180: 'soundtracks',\n",
       " 181: 'camelot',\n",
       " 182: 'mxpx',\n",
       " 183: 'hallberg',\n",
       " 184: 'renfield',\n",
       " 185: 'courte',\n",
       " 186: 'eeriest',\n",
       " 187: 'alives',\n",
       " 188: 'angsty',\n",
       " 189: 'wallis',\n",
       " 190: 'wain',\n",
       " 191: 'sulu',\n",
       " 192: 'ratings',\n",
       " 193: 'anynomous',\n",
       " 194: 'jaipur',\n",
       " 195: 'ivans',\n",
       " 196: 'kareena',\n",
       " 197: 'ireland',\n",
       " 198: 'adeline',\n",
       " 199: 'buppie',\n",
       " 200: 'wolfman',\n",
       " 201: 'dazzles',\n",
       " 202: 'maegi',\n",
       " 203: 'casting',\n",
       " 204: 'pedal',\n",
       " 205: 'significances',\n",
       " 206: 'squawks',\n",
       " 207: 'lanisha',\n",
       " 208: 'schoolmate',\n",
       " 209: 'cda',\n",
       " 210: 'draub',\n",
       " 211: 'nenji',\n",
       " 212: 'friz',\n",
       " 213: 'bate',\n",
       " 214: 'deacon',\n",
       " 215: 'saloons',\n",
       " 216: 'horsewhips',\n",
       " 217: 'equaled',\n",
       " 218: 'capshaw',\n",
       " 219: 'punters',\n",
       " 220: 'srebrenica',\n",
       " 221: 'unspecific',\n",
       " 222: 'auteurist',\n",
       " 223: 'gobledegook',\n",
       " 224: 'colossal',\n",
       " 225: 'unhittable',\n",
       " 226: 'snipering',\n",
       " 227: 'luckiest',\n",
       " 228: 'nyfiken',\n",
       " 229: 'beutiful',\n",
       " 230: 'fart',\n",
       " 231: 'misbegotten',\n",
       " 232: 'generating',\n",
       " 233: 'adamos',\n",
       " 234: 'somnath',\n",
       " 235: 'fabrication',\n",
       " 236: 'nataile',\n",
       " 237: 'hecq',\n",
       " 238: 'vamping',\n",
       " 239: 'surrendered',\n",
       " 240: 'fitzs',\n",
       " 241: 'masses',\n",
       " 242: 'mayweather',\n",
       " 243: 'specially',\n",
       " 244: 'overruled',\n",
       " 245: 'superspy',\n",
       " 246: 'earhole',\n",
       " 247: 'variety',\n",
       " 248: 'oral',\n",
       " 249: 'copyist',\n",
       " 250: 'wised',\n",
       " 251: 'inters',\n",
       " 252: 'hantz',\n",
       " 253: 'immorally',\n",
       " 254: 'barkin',\n",
       " 255: 'romanticize',\n",
       " 256: 'briefly',\n",
       " 257: 'cannibals',\n",
       " 258: 'sonali',\n",
       " 259: 'tinges',\n",
       " 260: 'prospers',\n",
       " 261: 'atem',\n",
       " 262: 'jaeckel',\n",
       " 263: 'overlays',\n",
       " 264: 'ruths',\n",
       " 265: 'relentlessly',\n",
       " 266: 'wrestling',\n",
       " 267: 'hereabouts',\n",
       " 268: 'hydros',\n",
       " 269: 'renters',\n",
       " 270: 'desplechin',\n",
       " 271: 'hankshaw',\n",
       " 272: 'moskowitz',\n",
       " 273: 'symphonie',\n",
       " 274: 'y',\n",
       " 275: 'boarders',\n",
       " 276: 'jefferey',\n",
       " 277: 'doc',\n",
       " 278: 'stroh',\n",
       " 279: 'haid',\n",
       " 280: 'farts',\n",
       " 281: 'kruegar',\n",
       " 282: 'raunch',\n",
       " 283: 'gestured',\n",
       " 284: 'ek',\n",
       " 285: 'wisely',\n",
       " 286: 'articulation',\n",
       " 287: 'haseena',\n",
       " 288: 'nombre',\n",
       " 289: 'threads',\n",
       " 290: 'aicha',\n",
       " 291: 'frakking',\n",
       " 292: 'quenton',\n",
       " 293: 'blobs',\n",
       " 294: 'traction',\n",
       " 295: 'chagossian',\n",
       " 296: 'eyesight',\n",
       " 297: 'dud',\n",
       " 298: 'funky',\n",
       " 299: 'reform',\n",
       " 300: 'disturb',\n",
       " 301: 'wellbalanced',\n",
       " 302: 'brats',\n",
       " 303: 'benoit',\n",
       " 304: 'encompassing',\n",
       " 305: 'swift',\n",
       " 306: 'variable',\n",
       " 307: 'leprosy',\n",
       " 308: 'scrapping',\n",
       " 309: 'jonesy',\n",
       " 310: 'confuddled',\n",
       " 311: 'meathead',\n",
       " 312: 'metropolitain',\n",
       " 313: 'culturally',\n",
       " 314: 'turntable',\n",
       " 315: 'gellar',\n",
       " 316: 'danver',\n",
       " 317: 'jutras',\n",
       " 318: 'dangling',\n",
       " 319: 'forgeries',\n",
       " 320: 'oakies',\n",
       " 321: 'blowhards',\n",
       " 322: 'crackle',\n",
       " 323: 'xd',\n",
       " 324: 'parsed',\n",
       " 325: 'patsy',\n",
       " 326: 'patiently',\n",
       " 327: 'wertmueller',\n",
       " 328: 'lazio',\n",
       " 329: 'zhigang',\n",
       " 330: 'reawakens',\n",
       " 331: 'cebuano',\n",
       " 332: 'animie',\n",
       " 333: 'kwouk',\n",
       " 334: 'wagnerites',\n",
       " 335: 'brainlessly',\n",
       " 336: 'carreyesque',\n",
       " 337: 'disappointmented',\n",
       " 338: 'racial',\n",
       " 339: 'bancroft',\n",
       " 340: 'launches',\n",
       " 341: 'bridget',\n",
       " 342: 'afros',\n",
       " 343: 'begotten',\n",
       " 344: 'lovelife',\n",
       " 345: 'bulldozer',\n",
       " 346: 'jurassik',\n",
       " 347: 'njosnavelin',\n",
       " 348: 'crosseyed',\n",
       " 349: 'preexisting',\n",
       " 350: 'midwinter',\n",
       " 351: 'cornbluth',\n",
       " 352: 'macabra',\n",
       " 353: 'francen',\n",
       " 354: 'soundbites',\n",
       " 355: 'naval',\n",
       " 356: 'amplify',\n",
       " 357: 'vital',\n",
       " 358: 'alley',\n",
       " 359: 'vd',\n",
       " 360: 'oneness',\n",
       " 361: 'modern',\n",
       " 362: 'justice',\n",
       " 363: 'productive',\n",
       " 364: 'cannavale',\n",
       " 365: 'philosopher',\n",
       " 366: 'novelty',\n",
       " 367: 'syriana',\n",
       " 368: 'shater',\n",
       " 369: 'traveller',\n",
       " 370: 'paranormal',\n",
       " 371: 'cakes',\n",
       " 372: 'bumptious',\n",
       " 373: 'radiator',\n",
       " 374: 'juarez',\n",
       " 375: 'vulnerability',\n",
       " 376: 'software',\n",
       " 377: 'denueve',\n",
       " 378: 'jacque',\n",
       " 379: 'tehmul',\n",
       " 380: 'tit',\n",
       " 381: 'hijixn',\n",
       " 382: 'bafta',\n",
       " 383: 'cristiano',\n",
       " 384: 'unjustified',\n",
       " 385: 'blooey',\n",
       " 386: 'satirise',\n",
       " 387: 'timereally',\n",
       " 388: 'cinematographe',\n",
       " 389: 'jaubert',\n",
       " 390: 'envision',\n",
       " 391: 'exquisitely',\n",
       " 392: 'preoccupied',\n",
       " 393: 'michalakis',\n",
       " 394: 'aiden',\n",
       " 395: 'agar',\n",
       " 396: 'audio',\n",
       " 397: 'dachshunds',\n",
       " 398: 'sanata',\n",
       " 399: 'industrious',\n",
       " 400: 'marvellously',\n",
       " 401: 'hated',\n",
       " 402: 'standardization',\n",
       " 403: 'snub',\n",
       " 404: 'stuffing',\n",
       " 405: 'gotham',\n",
       " 406: 'toland',\n",
       " 407: 'gamblers',\n",
       " 408: 'sino',\n",
       " 409: 'doorstop',\n",
       " 410: 'progressing',\n",
       " 411: 'strategy',\n",
       " 412: 'bathe',\n",
       " 413: 'cabal',\n",
       " 414: 'east',\n",
       " 415: 'verizon',\n",
       " 416: 'skycaptain',\n",
       " 417: 'konchalovksy',\n",
       " 418: 'massively',\n",
       " 419: 'bookkeeper',\n",
       " 420: 'walton',\n",
       " 421: 'doreen',\n",
       " 422: 'malahide',\n",
       " 423: 'hippos',\n",
       " 424: 'prisons',\n",
       " 425: 'lemongelli',\n",
       " 426: 'motionless',\n",
       " 427: 'adoration',\n",
       " 428: 'kolchack',\n",
       " 429: 'deedlit',\n",
       " 430: 'evacuates',\n",
       " 431: 'reconfirmed',\n",
       " 432: 'engross',\n",
       " 433: 'chester',\n",
       " 434: 'spacewalk',\n",
       " 435: 'guitarists',\n",
       " 436: 'antonyms',\n",
       " 437: 'outdrawing',\n",
       " 438: 'vaseekaramaana',\n",
       " 439: 'unessential',\n",
       " 440: 'priestley',\n",
       " 441: 'pristine',\n",
       " 442: 'loyalism',\n",
       " 443: 'ambigious',\n",
       " 444: 'undoing',\n",
       " 445: 'violated',\n",
       " 446: 'perc',\n",
       " 447: 'funkions',\n",
       " 448: 'changwei',\n",
       " 449: 'luggage',\n",
       " 450: 'exaggerated',\n",
       " 451: 'swanson',\n",
       " 452: 'spends',\n",
       " 453: 'philippines',\n",
       " 454: 'pretension',\n",
       " 455: 'breached',\n",
       " 456: 'jalapeno',\n",
       " 457: 'execration',\n",
       " 458: 'coprophilia',\n",
       " 459: 'typhoon',\n",
       " 460: 'slake',\n",
       " 461: 'ninga',\n",
       " 462: 'lavished',\n",
       " 463: 'cryptozoology',\n",
       " 464: 'darkness',\n",
       " 465: 'jeckyll',\n",
       " 466: 'barrages',\n",
       " 467: 'newswriter',\n",
       " 468: 'ills',\n",
       " 469: 'moshpit',\n",
       " 470: 'lesbianism',\n",
       " 471: 'imdbs',\n",
       " 472: 'metro',\n",
       " 473: 'aristotelian',\n",
       " 474: 'contemporaneity',\n",
       " 475: 'voluble',\n",
       " 476: 'hallucinations',\n",
       " 477: 'kanmuri',\n",
       " 478: 'borders',\n",
       " 479: 'genocidal',\n",
       " 480: 'bungling',\n",
       " 481: 'tolls',\n",
       " 482: 'casket',\n",
       " 483: 'partied',\n",
       " 484: 'surgery',\n",
       " 485: 'deepens',\n",
       " 486: 'kaufman',\n",
       " 487: 'pump',\n",
       " 488: 'swinged',\n",
       " 489: 'widespread',\n",
       " 490: 'sexa',\n",
       " 491: 'itfa',\n",
       " 492: 'implicitly',\n",
       " 493: 'outs',\n",
       " 494: 'misuse',\n",
       " 495: 'contended',\n",
       " 496: 'smithonites',\n",
       " 497: 'wrecker',\n",
       " 498: 'ylva',\n",
       " 499: 'acker',\n",
       " 500: 'burly',\n",
       " 501: 'millennium',\n",
       " 502: 'motorbikes',\n",
       " 503: 'nausicca',\n",
       " 504: 'gymakta',\n",
       " 505: 'structuralism',\n",
       " 506: 'chaser',\n",
       " 507: 'depleting',\n",
       " 508: 'hedlund',\n",
       " 509: 'bowman',\n",
       " 510: 'properly',\n",
       " 511: 'selection',\n",
       " 512: 'heed',\n",
       " 513: 'silicons',\n",
       " 514: 'lightheartedly',\n",
       " 515: 'hecklers',\n",
       " 516: 'inhaled',\n",
       " 517: 'humanize',\n",
       " 518: 'porker',\n",
       " 519: 'ancona',\n",
       " 520: 'abyss',\n",
       " 521: 'scattered',\n",
       " 522: 'zukovic',\n",
       " 523: 'inconsisties',\n",
       " 524: 'gs',\n",
       " 525: 'bonanzas',\n",
       " 526: 'cessation',\n",
       " 527: 'damiella',\n",
       " 528: 'boston',\n",
       " 529: 'hummer',\n",
       " 530: 'untucked',\n",
       " 531: 'commercisliation',\n",
       " 532: 'ungoriest',\n",
       " 533: 'lectern',\n",
       " 534: 'slag',\n",
       " 535: 'sagramore',\n",
       " 536: 'artigot',\n",
       " 537: 'writr',\n",
       " 538: 'peanut',\n",
       " 539: 'nist',\n",
       " 540: 'muddy',\n",
       " 541: 'gratification',\n",
       " 542: 'lipper',\n",
       " 543: 'randon',\n",
       " 544: 'impelled',\n",
       " 545: 'blatch',\n",
       " 546: 'mag',\n",
       " 547: 'mexicanenglish',\n",
       " 548: 'naswip',\n",
       " 549: 'surgically',\n",
       " 550: 'nearne',\n",
       " 551: 'comported',\n",
       " 552: 'analyzes',\n",
       " 553: 'chemotrodes',\n",
       " 554: 'lifestory',\n",
       " 555: 'dormants',\n",
       " 556: 'empires',\n",
       " 557: 'sparce',\n",
       " 558: 'danni',\n",
       " 559: 'throttle',\n",
       " 560: 'cabbages',\n",
       " 561: 'quizzical',\n",
       " 562: 'ibria',\n",
       " 563: 'rollan',\n",
       " 564: 'advertisers',\n",
       " 565: 'xia',\n",
       " 566: 'pockets',\n",
       " 567: 'witnessed',\n",
       " 568: 'innapropriate',\n",
       " 569: 'farrely',\n",
       " 570: 'interference',\n",
       " 571: 'afleck',\n",
       " 572: 'positivism',\n",
       " 573: 'colombo',\n",
       " 574: 'principle',\n",
       " 575: 'jaa',\n",
       " 576: 'balkanic',\n",
       " 577: 'welker',\n",
       " 578: 'misfitted',\n",
       " 579: 'shrift',\n",
       " 580: 'enunciates',\n",
       " 581: 'crazier',\n",
       " 582: 'engvall',\n",
       " 583: 'secretsdirector',\n",
       " 584: 'spat',\n",
       " 585: 'andalusia',\n",
       " 586: 'motivating',\n",
       " 587: 'greensleeves',\n",
       " 588: 'mccrea',\n",
       " 589: 'phobic',\n",
       " 590: 'dormael',\n",
       " 591: 'aguila',\n",
       " 592: 'terra',\n",
       " 593: 'younger',\n",
       " 594: 'upgraded',\n",
       " 595: 'brandner',\n",
       " 596: 'eglimata',\n",
       " 597: 'lyrics',\n",
       " 598: 'bundle',\n",
       " 599: 'endows',\n",
       " 600: 'compose',\n",
       " 601: 'pointed',\n",
       " 602: 'oooooozzzzzzed',\n",
       " 603: 'slavishly',\n",
       " 604: 'neckett',\n",
       " 605: 'coy',\n",
       " 606: 'overestimated',\n",
       " 607: 'vulkan',\n",
       " 608: 'kato',\n",
       " 609: 'blanketing',\n",
       " 610: 'polluting',\n",
       " 611: 'endure',\n",
       " 612: 'zack',\n",
       " 613: 'bloodied',\n",
       " 614: 'allyson',\n",
       " 615: 'uma',\n",
       " 616: 'michonoku',\n",
       " 617: 'sogo',\n",
       " 618: 'norsk',\n",
       " 619: 'explicit',\n",
       " 620: 'imbred',\n",
       " 621: 'sette',\n",
       " 622: 'pigeons',\n",
       " 623: 'lofts',\n",
       " 624: 'teordoro',\n",
       " 625: 'panoramic',\n",
       " 626: 'skeksis',\n",
       " 627: 'climb',\n",
       " 628: 'vivaah',\n",
       " 629: 'brush',\n",
       " 630: 'volition',\n",
       " 631: 'alesia',\n",
       " 632: 'disburses',\n",
       " 633: 'unbalance',\n",
       " 634: 'birnam',\n",
       " 635: 'arne',\n",
       " 636: 'underside',\n",
       " 637: 'casualties',\n",
       " 638: 'mabuse',\n",
       " 639: 'jogando',\n",
       " 640: 'handles',\n",
       " 641: 'scorned',\n",
       " 642: 'comensurate',\n",
       " 643: 'pacts',\n",
       " 644: 'bashers',\n",
       " 645: 'lifer',\n",
       " 646: 'seidelman',\n",
       " 647: 'barres',\n",
       " 648: 'salo',\n",
       " 649: 'pedestrians',\n",
       " 650: 'snowflake',\n",
       " 651: 'macha',\n",
       " 652: 'rino',\n",
       " 653: 'glady',\n",
       " 654: 'shutdown',\n",
       " 655: 'lonely',\n",
       " 656: 'cuts',\n",
       " 657: 'koontz',\n",
       " 658: 'husk',\n",
       " 659: 'beastmaster',\n",
       " 660: 'dirge',\n",
       " 661: 'ripley',\n",
       " 662: 'odette',\n",
       " 663: 'bwahahahahha',\n",
       " 664: 'float',\n",
       " 665: 'tete',\n",
       " 666: 'auxiliary',\n",
       " 667: 'effette',\n",
       " 668: 'caulder',\n",
       " 669: 'amber',\n",
       " 670: 'odysse',\n",
       " 671: 'satelite',\n",
       " 672: 'debucourt',\n",
       " 673: 'tolkien',\n",
       " 674: 'falak',\n",
       " 675: 'sweeny',\n",
       " 676: 'tracks',\n",
       " 677: 'voyager',\n",
       " 678: 'mildred',\n",
       " 679: 'nyu',\n",
       " 680: 'headlight',\n",
       " 681: 'outfit',\n",
       " 682: 'girlishness',\n",
       " 683: 'nietzcheans',\n",
       " 684: 'radiantly',\n",
       " 685: 'ennobling',\n",
       " 686: 'reminders',\n",
       " 687: 'plowed',\n",
       " 688: 'schedulers',\n",
       " 689: 'overmuch',\n",
       " 690: 'fratelli',\n",
       " 691: 'reportage',\n",
       " 692: 'cardinal',\n",
       " 693: 'atoned',\n",
       " 694: 'times',\n",
       " 695: 'oversimply',\n",
       " 696: 'jobs',\n",
       " 697: 'reichstagsbuilding',\n",
       " 698: 'hails',\n",
       " 699: 'final',\n",
       " 700: 'cheree',\n",
       " 701: 'subtract',\n",
       " 702: 'logging',\n",
       " 703: 'annexed',\n",
       " 704: 'sssss',\n",
       " 705: 'ground',\n",
       " 706: 'bowdlerized',\n",
       " 707: 'titledunsolved',\n",
       " 708: 'orin',\n",
       " 709: 'wiarton',\n",
       " 710: 'afforded',\n",
       " 711: 'skeptic',\n",
       " 712: 'weaponry',\n",
       " 713: 'immigrant',\n",
       " 714: 'filmatography',\n",
       " 715: 'bazaar',\n",
       " 716: 'lindy',\n",
       " 717: 'abkani',\n",
       " 718: 'mccree',\n",
       " 719: 'redubbed',\n",
       " 720: 'cgi',\n",
       " 721: 'dakota',\n",
       " 722: 'termed',\n",
       " 723: 'janowski',\n",
       " 724: 'espoir',\n",
       " 725: 'sinthome',\n",
       " 726: 'woche',\n",
       " 727: 'gariazzo',\n",
       " 728: 'ludicrious',\n",
       " 729: 'saleswomen',\n",
       " 730: 'annabeth',\n",
       " 731: 'usenet',\n",
       " 732: 'immediacy',\n",
       " 733: 'backlashes',\n",
       " 734: 'sarasohn',\n",
       " 735: 'explainable',\n",
       " 736: 'suburban',\n",
       " 737: 'drab',\n",
       " 738: 'changin',\n",
       " 739: 'finese',\n",
       " 740: 'boring',\n",
       " 741: 'manlis',\n",
       " 742: 'bgr',\n",
       " 743: 'dozes',\n",
       " 744: 'punkah',\n",
       " 745: 'tweedy',\n",
       " 746: 'chandulal',\n",
       " 747: 'beholder',\n",
       " 748: 'dkd',\n",
       " 749: 'anethesia',\n",
       " 750: 'classically',\n",
       " 751: 'levelled',\n",
       " 752: 'twentysomething',\n",
       " 753: 'rosentrasse',\n",
       " 754: 'doody',\n",
       " 755: 'badat',\n",
       " 756: 'jackhammers',\n",
       " 757: 'corny',\n",
       " 758: 'obsess',\n",
       " 759: 'lothar',\n",
       " 760: 'gymnastics',\n",
       " 761: 'utilitarian',\n",
       " 762: 'connors',\n",
       " 763: 'lameass',\n",
       " 764: 'guadalcanal',\n",
       " 765: 'scenic',\n",
       " 766: 'existed',\n",
       " 767: 'hasslehoff',\n",
       " 768: 'regan',\n",
       " 769: 'aborted',\n",
       " 770: 'moody',\n",
       " 771: 'stallonethat',\n",
       " 772: 'avantegardistic',\n",
       " 773: 'dont',\n",
       " 774: 'chatterboxes',\n",
       " 775: 'rosetto',\n",
       " 776: 'leer',\n",
       " 777: 'marionette',\n",
       " 778: 'monolithic',\n",
       " 779: 'firefly',\n",
       " 780: 'humphrey',\n",
       " 781: 'salaried',\n",
       " 782: 'fiedler',\n",
       " 783: 'farnsworth',\n",
       " 784: 'wyler',\n",
       " 785: 'pew',\n",
       " 786: 'heartened',\n",
       " 787: 'ruinous',\n",
       " 788: 'perth',\n",
       " 789: 'malibu',\n",
       " 790: 'ghostwriting',\n",
       " 791: 'sichuan',\n",
       " 792: 'noe',\n",
       " 793: 'boneheaded',\n",
       " 794: 'knowns',\n",
       " 795: 'eddie',\n",
       " 796: 'denmark',\n",
       " 797: 'thirtyish',\n",
       " 798: 'omnibusan',\n",
       " 799: 'musketeers',\n",
       " 800: 'actions',\n",
       " 801: 'raskolnikov',\n",
       " 802: 'smalltime',\n",
       " 803: 'crowne',\n",
       " 804: 'mel',\n",
       " 805: 'flier',\n",
       " 806: 'athletes',\n",
       " 807: 'yale',\n",
       " 808: 'persistance',\n",
       " 809: 'fkers',\n",
       " 810: 'germanish',\n",
       " 811: 'brang',\n",
       " 812: 'lisbeth',\n",
       " 813: 'empathy',\n",
       " 814: 'prevails',\n",
       " 815: 'yamadera',\n",
       " 816: 'auctions',\n",
       " 817: 'deviating',\n",
       " 818: 'aicn',\n",
       " 819: 'paternity',\n",
       " 820: 'gearing',\n",
       " 821: 'johnsons',\n",
       " 822: 'sobre',\n",
       " 823: 'membury',\n",
       " 824: 'bookish',\n",
       " 825: 'inspecting',\n",
       " 826: 'fit',\n",
       " 827: 'euphemisms',\n",
       " 828: 'contempory',\n",
       " 829: 'refined',\n",
       " 830: 'bodybuilders',\n",
       " 831: 'posers',\n",
       " 832: 'moloney',\n",
       " 833: 'pilling',\n",
       " 834: 'fictionalising',\n",
       " 835: 'arliss',\n",
       " 836: 'haven',\n",
       " 837: 'z',\n",
       " 838: 'melchior',\n",
       " 839: 'retains',\n",
       " 840: 'segregating',\n",
       " 841: 'coptic',\n",
       " 842: 'bute',\n",
       " 843: 'hostesses',\n",
       " 844: 'deducing',\n",
       " 845: 'realist',\n",
       " 846: 'depicted',\n",
       " 847: 'egger',\n",
       " 848: 'pals',\n",
       " 849: 'cazal',\n",
       " 850: 'bamboo',\n",
       " 851: 'tilted',\n",
       " 852: 'idiosyncrasy',\n",
       " 853: 'tersteeghe',\n",
       " 854: 'tunes',\n",
       " 855: 'iggy',\n",
       " 856: 'escorting',\n",
       " 857: 'absorbed',\n",
       " 858: 'homoeroticism',\n",
       " 859: 'illiterate',\n",
       " 860: 'jumpstart',\n",
       " 861: 'zhang',\n",
       " 862: 'carrying',\n",
       " 863: 'childhoods',\n",
       " 864: 'stillness',\n",
       " 865: 'lippman',\n",
       " 866: 'midsts',\n",
       " 867: 'dornwinkle',\n",
       " 868: 'kliegs',\n",
       " 869: 'beavis',\n",
       " 870: 'mathematicians',\n",
       " 871: 'devoreaux',\n",
       " 872: 'kutchek',\n",
       " 873: 'grotesquesat',\n",
       " 874: 'bamboozling',\n",
       " 875: 'unimaginative',\n",
       " 876: 'raymond',\n",
       " 877: 'tyron',\n",
       " 878: 'chilean',\n",
       " 879: 'rhind',\n",
       " 880: 'seftel',\n",
       " 881: 'thespic',\n",
       " 882: 'etches',\n",
       " 883: 'hangs',\n",
       " 884: 'bergdof',\n",
       " 885: 'tire',\n",
       " 886: 'toady',\n",
       " 887: 'soured',\n",
       " 888: 'jox',\n",
       " 889: 'thesigner',\n",
       " 890: 'mie',\n",
       " 891: 'mishap',\n",
       " 892: 'declarative',\n",
       " 893: 'coincidence',\n",
       " 894: 'btchiness',\n",
       " 895: 'blessing',\n",
       " 896: 'aquires',\n",
       " 897: 'malcolm',\n",
       " 898: 'hailsham',\n",
       " 899: 'chop',\n",
       " 900: 'starters',\n",
       " 901: 'stats',\n",
       " 902: 'dereliction',\n",
       " 903: 'priding',\n",
       " 904: 'kumble',\n",
       " 905: 'nook',\n",
       " 906: 'show',\n",
       " 907: 'crystal',\n",
       " 908: 'transaction',\n",
       " 909: 'dweller',\n",
       " 910: 'panty',\n",
       " 911: 'bredell',\n",
       " 912: 'shiploads',\n",
       " 913: 'piznarski',\n",
       " 914: 'synthesizers',\n",
       " 915: 'incomprehensibility',\n",
       " 916: 'blissfully',\n",
       " 917: 'musicthe',\n",
       " 918: 'temporary',\n",
       " 919: 'nack',\n",
       " 920: 'grinchy',\n",
       " 921: 'twyker',\n",
       " 922: 'widening',\n",
       " 923: 'shakewspeare',\n",
       " 924: 'longhetti',\n",
       " 925: 'barred',\n",
       " 926: 'storytelling',\n",
       " 927: 'fixated',\n",
       " 928: 'davalos',\n",
       " 929: 'baseless',\n",
       " 930: 'frikkin',\n",
       " 931: 'baltimorean',\n",
       " 932: 'synacures',\n",
       " 933: 'riots',\n",
       " 934: 'slagged',\n",
       " 935: 'knuckler',\n",
       " 936: 'swedlow',\n",
       " 937: 'defect',\n",
       " 938: 'bmoviefreak',\n",
       " 939: 'electrician',\n",
       " 940: 'cloying',\n",
       " 941: 'tersely',\n",
       " 942: 'jillson',\n",
       " 943: 'gamecube',\n",
       " 944: 'helms',\n",
       " 945: 'loa',\n",
       " 946: 'subpoints',\n",
       " 947: 'psychic',\n",
       " 948: 'uggh',\n",
       " 949: 'metaphoric',\n",
       " 950: 'buch',\n",
       " 951: 'consenting',\n",
       " 952: 'pdf',\n",
       " 953: 'whacking',\n",
       " 954: 'slicked',\n",
       " 955: 'idealizing',\n",
       " 956: 'gathers',\n",
       " 957: 'guilts',\n",
       " 958: 'carapace',\n",
       " 959: 'ashcroft',\n",
       " 960: 'rendering',\n",
       " 961: 'hoopers',\n",
       " 962: 'mikl',\n",
       " 963: 'paige',\n",
       " 964: 'guillotines',\n",
       " 965: 'pko',\n",
       " 966: 'blankfield',\n",
       " 967: 'epiphany',\n",
       " 968: 'ally',\n",
       " 969: 'uttered',\n",
       " 970: 'aldridge',\n",
       " 971: 'never',\n",
       " 972: 'shimomo',\n",
       " 973: 'zacatecas',\n",
       " 974: 'winked',\n",
       " 975: 'supblot',\n",
       " 976: 'phineas',\n",
       " 977: 'object',\n",
       " 978: 'lagomorpha',\n",
       " 979: 'crappily',\n",
       " 980: 'expansionist',\n",
       " 981: 'hdn',\n",
       " 982: 'fanatic',\n",
       " 983: 'arranger',\n",
       " 984: 'degeneration',\n",
       " 985: 'holographic',\n",
       " 986: 'pfieffer',\n",
       " 987: 'calibanian',\n",
       " 988: 'meander',\n",
       " 989: 'library',\n",
       " 990: 'skitz',\n",
       " 991: 'theby',\n",
       " 992: 'quintana',\n",
       " 993: 'spinozean',\n",
       " 994: 'macfadyen',\n",
       " 995: 'zombiesnatch',\n",
       " 996: 'twitter',\n",
       " 997: 'glassy',\n",
       " 998: 'became',\n",
       " 999: 'dusenberry',\n",
       " 1000: 'motorised',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典，格式： 整数 ： 单词\n",
    "word_int = {w:int(i) for i, w in int_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'organic': 1,\n",
       " 'tigerland': 2,\n",
       " 'telescope': 3,\n",
       " 'mediocrities': 4,\n",
       " 'proto': 5,\n",
       " 'cutout': 6,\n",
       " 'tomlinson': 7,\n",
       " 'dhawan': 8,\n",
       " 'vegtigris': 9,\n",
       " 'ripner': 10,\n",
       " 'sly': 11,\n",
       " 'putated': 12,\n",
       " 'szwarc': 13,\n",
       " 'soviets': 14,\n",
       " 'brady': 15,\n",
       " 'improbabilities': 16,\n",
       " 'fearlessly': 17,\n",
       " 'cerletti': 18,\n",
       " 'psychiatrically': 19,\n",
       " 'investigates': 20,\n",
       " 'reptilian': 21,\n",
       " 'seawright': 22,\n",
       " 'tirol': 23,\n",
       " 'predicts': 24,\n",
       " 'arnis': 25,\n",
       " 'buttress': 26,\n",
       " 'jordowsky': 27,\n",
       " 'fairbanks': 28,\n",
       " 'everytime': 29,\n",
       " 'kolya': 30,\n",
       " 'francesco': 31,\n",
       " 'wen': 32,\n",
       " 'asset': 33,\n",
       " 'verson': 34,\n",
       " 'whines': 35,\n",
       " 'commishioner': 36,\n",
       " 'angelica': 37,\n",
       " 'teenagers': 38,\n",
       " 'pausing': 39,\n",
       " 'wkrp': 40,\n",
       " 'manpower': 41,\n",
       " 'chromium': 42,\n",
       " 'arkush': 43,\n",
       " 'darkly': 44,\n",
       " 'genes': 45,\n",
       " 'muere': 46,\n",
       " 'youand': 47,\n",
       " 'showcases': 48,\n",
       " 'pickman': 49,\n",
       " 'shumachers': 50,\n",
       " 'corneau': 51,\n",
       " 'enlivening': 52,\n",
       " 'invocus': 53,\n",
       " 'kringle': 54,\n",
       " 'sahi': 55,\n",
       " 'durden': 56,\n",
       " 'supermortalman': 57,\n",
       " 'mewing': 58,\n",
       " 'cowhand': 59,\n",
       " 'fillet': 60,\n",
       " 'grimmest': 61,\n",
       " 'coproduction': 62,\n",
       " 'gelatin': 63,\n",
       " 'retardedness': 64,\n",
       " 'heidijean': 65,\n",
       " 'frustrates': 66,\n",
       " 'talk': 67,\n",
       " 'local': 68,\n",
       " 'oaths': 69,\n",
       " 'shortens': 70,\n",
       " 'unholy': 71,\n",
       " 'travelers': 72,\n",
       " 'entendre': 73,\n",
       " 'petey': 74,\n",
       " 'bamrha': 75,\n",
       " 'unboring': 76,\n",
       " 'heuristic': 77,\n",
       " 'thieriot': 78,\n",
       " 'crockazilla': 79,\n",
       " 'convent': 80,\n",
       " 'overexplanation': 81,\n",
       " 'attachs': 82,\n",
       " 'restaurants': 83,\n",
       " 'monthly': 84,\n",
       " 'solves': 85,\n",
       " 'circuitry': 86,\n",
       " 'sarcastic': 87,\n",
       " 'chuckle': 88,\n",
       " 'larrikin': 89,\n",
       " 'bustle': 90,\n",
       " 'maman': 91,\n",
       " 'dramatisations': 92,\n",
       " 'cherubs': 93,\n",
       " 'crazes': 94,\n",
       " 'tramonti': 95,\n",
       " 'waxed': 96,\n",
       " 'uss': 97,\n",
       " 'silverstonesque': 98,\n",
       " 'malditos': 99,\n",
       " 'camped': 100,\n",
       " 'transmitted': 101,\n",
       " 'enormous': 102,\n",
       " 'cramp': 103,\n",
       " 'reminisces': 104,\n",
       " 'damiano': 105,\n",
       " 'copped': 106,\n",
       " 'bafflement': 107,\n",
       " 'opra': 108,\n",
       " 'shrink': 109,\n",
       " 'kostic': 110,\n",
       " 'caterers': 111,\n",
       " 'droned': 112,\n",
       " 'bhagam': 113,\n",
       " 'langford': 114,\n",
       " 'geisel': 115,\n",
       " 'bilitis': 116,\n",
       " 'cruse': 117,\n",
       " 'proves': 118,\n",
       " 'rekindling': 119,\n",
       " 'unforunatley': 120,\n",
       " 'thoughtful': 121,\n",
       " 'bucket': 122,\n",
       " 'diane': 123,\n",
       " 'grasping': 124,\n",
       " 'gloved': 125,\n",
       " 'superlow': 126,\n",
       " 'remedial': 127,\n",
       " 'piddles': 128,\n",
       " 'invested': 129,\n",
       " 'bypasses': 130,\n",
       " 'proletariat': 131,\n",
       " 'debi': 132,\n",
       " 'mommy': 133,\n",
       " 'swaps': 134,\n",
       " 'chasey': 135,\n",
       " 'animalshas': 136,\n",
       " 'beaute': 137,\n",
       " 'bates': 138,\n",
       " 'poppycock': 139,\n",
       " 'dim': 140,\n",
       " 'crooke': 141,\n",
       " 'testaverdi': 142,\n",
       " 'wheedle': 143,\n",
       " 'slumps': 144,\n",
       " 'sequels': 145,\n",
       " 'cuddlesome': 146,\n",
       " 'newer': 147,\n",
       " 'viewed': 148,\n",
       " 'willaims': 149,\n",
       " 'grandiose': 150,\n",
       " 'polygamy': 151,\n",
       " 'rebounding': 152,\n",
       " 'kidnappedin': 153,\n",
       " 'tomita': 154,\n",
       " 'biros': 155,\n",
       " 'benchley': 156,\n",
       " 'syncretism': 157,\n",
       " 'bullfighter': 158,\n",
       " 'garfiled': 159,\n",
       " 'shouldn': 160,\n",
       " 'fixation': 161,\n",
       " 'elope': 162,\n",
       " 'enveloped': 163,\n",
       " 'renewed': 164,\n",
       " 'carcass': 165,\n",
       " 'draws': 166,\n",
       " 'farlinger': 167,\n",
       " 'debuted': 168,\n",
       " 'charity': 169,\n",
       " 'divx': 170,\n",
       " 'kc': 171,\n",
       " 'novacaine': 172,\n",
       " 'buchman': 173,\n",
       " 'letdown': 174,\n",
       " 'kempo': 175,\n",
       " 'ellery': 176,\n",
       " 'classiest': 177,\n",
       " 'unused': 178,\n",
       " 'foywonder': 179,\n",
       " 'soundtracks': 180,\n",
       " 'camelot': 181,\n",
       " 'mxpx': 182,\n",
       " 'hallberg': 183,\n",
       " 'renfield': 184,\n",
       " 'courte': 185,\n",
       " 'eeriest': 186,\n",
       " 'alives': 187,\n",
       " 'angsty': 188,\n",
       " 'wallis': 189,\n",
       " 'wain': 190,\n",
       " 'sulu': 191,\n",
       " 'ratings': 192,\n",
       " 'anynomous': 193,\n",
       " 'jaipur': 194,\n",
       " 'ivans': 195,\n",
       " 'kareena': 196,\n",
       " 'ireland': 197,\n",
       " 'adeline': 198,\n",
       " 'buppie': 199,\n",
       " 'wolfman': 200,\n",
       " 'dazzles': 201,\n",
       " 'maegi': 202,\n",
       " 'casting': 203,\n",
       " 'pedal': 204,\n",
       " 'significances': 205,\n",
       " 'squawks': 206,\n",
       " 'lanisha': 207,\n",
       " 'schoolmate': 208,\n",
       " 'cda': 209,\n",
       " 'draub': 210,\n",
       " 'nenji': 211,\n",
       " 'friz': 212,\n",
       " 'bate': 213,\n",
       " 'deacon': 214,\n",
       " 'saloons': 215,\n",
       " 'horsewhips': 216,\n",
       " 'equaled': 217,\n",
       " 'capshaw': 218,\n",
       " 'punters': 219,\n",
       " 'srebrenica': 220,\n",
       " 'unspecific': 221,\n",
       " 'auteurist': 222,\n",
       " 'gobledegook': 223,\n",
       " 'colossal': 224,\n",
       " 'unhittable': 225,\n",
       " 'snipering': 226,\n",
       " 'luckiest': 227,\n",
       " 'nyfiken': 228,\n",
       " 'beutiful': 229,\n",
       " 'fart': 230,\n",
       " 'misbegotten': 231,\n",
       " 'generating': 232,\n",
       " 'adamos': 233,\n",
       " 'somnath': 234,\n",
       " 'fabrication': 235,\n",
       " 'nataile': 236,\n",
       " 'hecq': 237,\n",
       " 'vamping': 238,\n",
       " 'surrendered': 239,\n",
       " 'fitzs': 240,\n",
       " 'masses': 241,\n",
       " 'mayweather': 242,\n",
       " 'specially': 243,\n",
       " 'overruled': 244,\n",
       " 'superspy': 245,\n",
       " 'earhole': 246,\n",
       " 'variety': 247,\n",
       " 'oral': 248,\n",
       " 'copyist': 249,\n",
       " 'wised': 250,\n",
       " 'inters': 251,\n",
       " 'hantz': 252,\n",
       " 'immorally': 253,\n",
       " 'barkin': 254,\n",
       " 'romanticize': 255,\n",
       " 'briefly': 256,\n",
       " 'cannibals': 257,\n",
       " 'sonali': 258,\n",
       " 'tinges': 259,\n",
       " 'prospers': 260,\n",
       " 'atem': 261,\n",
       " 'jaeckel': 262,\n",
       " 'overlays': 263,\n",
       " 'ruths': 264,\n",
       " 'relentlessly': 265,\n",
       " 'wrestling': 266,\n",
       " 'hereabouts': 267,\n",
       " 'hydros': 268,\n",
       " 'renters': 269,\n",
       " 'desplechin': 270,\n",
       " 'hankshaw': 271,\n",
       " 'moskowitz': 272,\n",
       " 'symphonie': 273,\n",
       " 'y': 274,\n",
       " 'boarders': 275,\n",
       " 'jefferey': 276,\n",
       " 'doc': 277,\n",
       " 'stroh': 278,\n",
       " 'haid': 279,\n",
       " 'farts': 280,\n",
       " 'kruegar': 281,\n",
       " 'raunch': 282,\n",
       " 'gestured': 283,\n",
       " 'ek': 284,\n",
       " 'wisely': 285,\n",
       " 'articulation': 286,\n",
       " 'haseena': 287,\n",
       " 'nombre': 288,\n",
       " 'threads': 289,\n",
       " 'aicha': 290,\n",
       " 'frakking': 291,\n",
       " 'quenton': 292,\n",
       " 'blobs': 293,\n",
       " 'traction': 294,\n",
       " 'chagossian': 295,\n",
       " 'eyesight': 296,\n",
       " 'dud': 297,\n",
       " 'funky': 298,\n",
       " 'reform': 299,\n",
       " 'disturb': 300,\n",
       " 'wellbalanced': 301,\n",
       " 'brats': 302,\n",
       " 'benoit': 303,\n",
       " 'encompassing': 304,\n",
       " 'swift': 305,\n",
       " 'variable': 306,\n",
       " 'leprosy': 307,\n",
       " 'scrapping': 308,\n",
       " 'jonesy': 309,\n",
       " 'confuddled': 310,\n",
       " 'meathead': 311,\n",
       " 'metropolitain': 312,\n",
       " 'culturally': 313,\n",
       " 'turntable': 314,\n",
       " 'gellar': 315,\n",
       " 'danver': 316,\n",
       " 'jutras': 317,\n",
       " 'dangling': 318,\n",
       " 'forgeries': 319,\n",
       " 'oakies': 320,\n",
       " 'blowhards': 321,\n",
       " 'crackle': 322,\n",
       " 'xd': 323,\n",
       " 'parsed': 324,\n",
       " 'patsy': 325,\n",
       " 'patiently': 326,\n",
       " 'wertmueller': 327,\n",
       " 'lazio': 328,\n",
       " 'zhigang': 329,\n",
       " 'reawakens': 330,\n",
       " 'cebuano': 331,\n",
       " 'animie': 332,\n",
       " 'kwouk': 333,\n",
       " 'wagnerites': 334,\n",
       " 'brainlessly': 335,\n",
       " 'carreyesque': 336,\n",
       " 'disappointmented': 337,\n",
       " 'racial': 338,\n",
       " 'bancroft': 339,\n",
       " 'launches': 340,\n",
       " 'bridget': 341,\n",
       " 'afros': 342,\n",
       " 'begotten': 343,\n",
       " 'lovelife': 344,\n",
       " 'bulldozer': 345,\n",
       " 'jurassik': 346,\n",
       " 'njosnavelin': 347,\n",
       " 'crosseyed': 348,\n",
       " 'preexisting': 349,\n",
       " 'midwinter': 350,\n",
       " 'cornbluth': 351,\n",
       " 'macabra': 352,\n",
       " 'francen': 353,\n",
       " 'soundbites': 354,\n",
       " 'naval': 355,\n",
       " 'amplify': 356,\n",
       " 'vital': 357,\n",
       " 'alley': 358,\n",
       " 'vd': 359,\n",
       " 'oneness': 360,\n",
       " 'modern': 361,\n",
       " 'justice': 362,\n",
       " 'productive': 363,\n",
       " 'cannavale': 364,\n",
       " 'philosopher': 365,\n",
       " 'novelty': 366,\n",
       " 'syriana': 367,\n",
       " 'shater': 368,\n",
       " 'traveller': 369,\n",
       " 'paranormal': 370,\n",
       " 'cakes': 371,\n",
       " 'bumptious': 372,\n",
       " 'radiator': 373,\n",
       " 'juarez': 374,\n",
       " 'vulnerability': 375,\n",
       " 'software': 376,\n",
       " 'denueve': 377,\n",
       " 'jacque': 378,\n",
       " 'tehmul': 379,\n",
       " 'tit': 380,\n",
       " 'hijixn': 381,\n",
       " 'bafta': 382,\n",
       " 'cristiano': 383,\n",
       " 'unjustified': 384,\n",
       " 'blooey': 385,\n",
       " 'satirise': 386,\n",
       " 'timereally': 387,\n",
       " 'cinematographe': 388,\n",
       " 'jaubert': 389,\n",
       " 'envision': 390,\n",
       " 'exquisitely': 391,\n",
       " 'preoccupied': 392,\n",
       " 'michalakis': 393,\n",
       " 'aiden': 394,\n",
       " 'agar': 395,\n",
       " 'audio': 396,\n",
       " 'dachshunds': 397,\n",
       " 'sanata': 398,\n",
       " 'industrious': 399,\n",
       " 'marvellously': 400,\n",
       " 'hated': 401,\n",
       " 'standardization': 402,\n",
       " 'snub': 403,\n",
       " 'stuffing': 404,\n",
       " 'gotham': 405,\n",
       " 'toland': 406,\n",
       " 'gamblers': 407,\n",
       " 'sino': 408,\n",
       " 'doorstop': 409,\n",
       " 'progressing': 410,\n",
       " 'strategy': 411,\n",
       " 'bathe': 412,\n",
       " 'cabal': 413,\n",
       " 'east': 414,\n",
       " 'verizon': 415,\n",
       " 'skycaptain': 416,\n",
       " 'konchalovksy': 417,\n",
       " 'massively': 418,\n",
       " 'bookkeeper': 419,\n",
       " 'walton': 420,\n",
       " 'doreen': 421,\n",
       " 'malahide': 422,\n",
       " 'hippos': 423,\n",
       " 'prisons': 424,\n",
       " 'lemongelli': 425,\n",
       " 'motionless': 426,\n",
       " 'adoration': 427,\n",
       " 'kolchack': 428,\n",
       " 'deedlit': 429,\n",
       " 'evacuates': 430,\n",
       " 'reconfirmed': 431,\n",
       " 'engross': 432,\n",
       " 'chester': 433,\n",
       " 'spacewalk': 434,\n",
       " 'guitarists': 435,\n",
       " 'antonyms': 436,\n",
       " 'outdrawing': 437,\n",
       " 'vaseekaramaana': 438,\n",
       " 'unessential': 439,\n",
       " 'priestley': 440,\n",
       " 'pristine': 441,\n",
       " 'loyalism': 442,\n",
       " 'ambigious': 443,\n",
       " 'undoing': 444,\n",
       " 'violated': 445,\n",
       " 'perc': 446,\n",
       " 'funkions': 447,\n",
       " 'changwei': 448,\n",
       " 'luggage': 449,\n",
       " 'exaggerated': 450,\n",
       " 'swanson': 451,\n",
       " 'spends': 452,\n",
       " 'philippines': 453,\n",
       " 'pretension': 454,\n",
       " 'breached': 455,\n",
       " 'jalapeno': 456,\n",
       " 'execration': 457,\n",
       " 'coprophilia': 458,\n",
       " 'typhoon': 459,\n",
       " 'slake': 460,\n",
       " 'ninga': 461,\n",
       " 'lavished': 462,\n",
       " 'cryptozoology': 463,\n",
       " 'darkness': 464,\n",
       " 'jeckyll': 465,\n",
       " 'barrages': 466,\n",
       " 'newswriter': 467,\n",
       " 'ills': 468,\n",
       " 'moshpit': 469,\n",
       " 'lesbianism': 470,\n",
       " 'imdbs': 471,\n",
       " 'metro': 472,\n",
       " 'aristotelian': 473,\n",
       " 'contemporaneity': 474,\n",
       " 'voluble': 475,\n",
       " 'hallucinations': 476,\n",
       " 'kanmuri': 477,\n",
       " 'borders': 478,\n",
       " 'genocidal': 479,\n",
       " 'bungling': 480,\n",
       " 'tolls': 481,\n",
       " 'casket': 482,\n",
       " 'partied': 483,\n",
       " 'surgery': 484,\n",
       " 'deepens': 485,\n",
       " 'kaufman': 486,\n",
       " 'pump': 487,\n",
       " 'swinged': 488,\n",
       " 'widespread': 489,\n",
       " 'sexa': 490,\n",
       " 'itfa': 491,\n",
       " 'implicitly': 492,\n",
       " 'outs': 493,\n",
       " 'misuse': 494,\n",
       " 'contended': 495,\n",
       " 'smithonites': 496,\n",
       " 'wrecker': 497,\n",
       " 'ylva': 498,\n",
       " 'acker': 499,\n",
       " 'burly': 500,\n",
       " 'millennium': 501,\n",
       " 'motorbikes': 502,\n",
       " 'nausicca': 503,\n",
       " 'gymakta': 504,\n",
       " 'structuralism': 505,\n",
       " 'chaser': 506,\n",
       " 'depleting': 507,\n",
       " 'hedlund': 508,\n",
       " 'bowman': 509,\n",
       " 'properly': 510,\n",
       " 'selection': 511,\n",
       " 'heed': 512,\n",
       " 'silicons': 513,\n",
       " 'lightheartedly': 514,\n",
       " 'hecklers': 515,\n",
       " 'inhaled': 516,\n",
       " 'humanize': 517,\n",
       " 'porker': 518,\n",
       " 'ancona': 519,\n",
       " 'abyss': 520,\n",
       " 'scattered': 521,\n",
       " 'zukovic': 522,\n",
       " 'inconsisties': 523,\n",
       " 'gs': 524,\n",
       " 'bonanzas': 525,\n",
       " 'cessation': 526,\n",
       " 'damiella': 527,\n",
       " 'boston': 528,\n",
       " 'hummer': 529,\n",
       " 'untucked': 530,\n",
       " 'commercisliation': 531,\n",
       " 'ungoriest': 532,\n",
       " 'lectern': 533,\n",
       " 'slag': 534,\n",
       " 'sagramore': 535,\n",
       " 'artigot': 536,\n",
       " 'writr': 537,\n",
       " 'peanut': 538,\n",
       " 'nist': 539,\n",
       " 'muddy': 540,\n",
       " 'gratification': 541,\n",
       " 'lipper': 542,\n",
       " 'randon': 543,\n",
       " 'impelled': 544,\n",
       " 'blatch': 545,\n",
       " 'mag': 546,\n",
       " 'mexicanenglish': 547,\n",
       " 'naswip': 548,\n",
       " 'surgically': 549,\n",
       " 'nearne': 550,\n",
       " 'comported': 551,\n",
       " 'analyzes': 552,\n",
       " 'chemotrodes': 553,\n",
       " 'lifestory': 554,\n",
       " 'dormants': 555,\n",
       " 'empires': 556,\n",
       " 'sparce': 557,\n",
       " 'danni': 558,\n",
       " 'throttle': 559,\n",
       " 'cabbages': 560,\n",
       " 'quizzical': 561,\n",
       " 'ibria': 562,\n",
       " 'rollan': 563,\n",
       " 'advertisers': 564,\n",
       " 'xia': 565,\n",
       " 'pockets': 566,\n",
       " 'witnessed': 567,\n",
       " 'innapropriate': 568,\n",
       " 'farrely': 569,\n",
       " 'interference': 570,\n",
       " 'afleck': 571,\n",
       " 'positivism': 572,\n",
       " 'colombo': 573,\n",
       " 'principle': 574,\n",
       " 'jaa': 575,\n",
       " 'balkanic': 576,\n",
       " 'welker': 577,\n",
       " 'misfitted': 578,\n",
       " 'shrift': 579,\n",
       " 'enunciates': 580,\n",
       " 'crazier': 581,\n",
       " 'engvall': 582,\n",
       " 'secretsdirector': 583,\n",
       " 'spat': 584,\n",
       " 'andalusia': 585,\n",
       " 'motivating': 586,\n",
       " 'greensleeves': 587,\n",
       " 'mccrea': 588,\n",
       " 'phobic': 589,\n",
       " 'dormael': 590,\n",
       " 'aguila': 591,\n",
       " 'terra': 592,\n",
       " 'younger': 593,\n",
       " 'upgraded': 594,\n",
       " 'brandner': 595,\n",
       " 'eglimata': 596,\n",
       " 'lyrics': 597,\n",
       " 'bundle': 598,\n",
       " 'endows': 599,\n",
       " 'compose': 600,\n",
       " 'pointed': 601,\n",
       " 'oooooozzzzzzed': 602,\n",
       " 'slavishly': 603,\n",
       " 'neckett': 604,\n",
       " 'coy': 605,\n",
       " 'overestimated': 606,\n",
       " 'vulkan': 607,\n",
       " 'kato': 608,\n",
       " 'blanketing': 609,\n",
       " 'polluting': 610,\n",
       " 'endure': 611,\n",
       " 'zack': 612,\n",
       " 'bloodied': 613,\n",
       " 'allyson': 614,\n",
       " 'uma': 615,\n",
       " 'michonoku': 616,\n",
       " 'sogo': 617,\n",
       " 'norsk': 618,\n",
       " 'explicit': 619,\n",
       " 'imbred': 620,\n",
       " 'sette': 621,\n",
       " 'pigeons': 622,\n",
       " 'lofts': 623,\n",
       " 'teordoro': 624,\n",
       " 'panoramic': 625,\n",
       " 'skeksis': 626,\n",
       " 'climb': 627,\n",
       " 'vivaah': 628,\n",
       " 'brush': 629,\n",
       " 'volition': 630,\n",
       " 'alesia': 631,\n",
       " 'disburses': 632,\n",
       " 'unbalance': 633,\n",
       " 'birnam': 634,\n",
       " 'arne': 635,\n",
       " 'underside': 636,\n",
       " 'casualties': 637,\n",
       " 'mabuse': 638,\n",
       " 'jogando': 639,\n",
       " 'handles': 640,\n",
       " 'scorned': 641,\n",
       " 'comensurate': 642,\n",
       " 'pacts': 643,\n",
       " 'bashers': 644,\n",
       " 'lifer': 645,\n",
       " 'seidelman': 646,\n",
       " 'barres': 647,\n",
       " 'salo': 648,\n",
       " 'pedestrians': 649,\n",
       " 'snowflake': 650,\n",
       " 'macha': 651,\n",
       " 'rino': 652,\n",
       " 'glady': 653,\n",
       " 'shutdown': 654,\n",
       " 'lonely': 655,\n",
       " 'cuts': 656,\n",
       " 'koontz': 657,\n",
       " 'husk': 658,\n",
       " 'beastmaster': 659,\n",
       " 'dirge': 660,\n",
       " 'ripley': 661,\n",
       " 'odette': 662,\n",
       " 'bwahahahahha': 663,\n",
       " 'float': 664,\n",
       " 'tete': 665,\n",
       " 'auxiliary': 666,\n",
       " 'effette': 667,\n",
       " 'caulder': 668,\n",
       " 'amber': 669,\n",
       " 'odysse': 670,\n",
       " 'satelite': 671,\n",
       " 'debucourt': 672,\n",
       " 'tolkien': 673,\n",
       " 'falak': 674,\n",
       " 'sweeny': 675,\n",
       " 'tracks': 676,\n",
       " 'voyager': 677,\n",
       " 'mildred': 678,\n",
       " 'nyu': 679,\n",
       " 'headlight': 680,\n",
       " 'outfit': 681,\n",
       " 'girlishness': 682,\n",
       " 'nietzcheans': 683,\n",
       " 'radiantly': 684,\n",
       " 'ennobling': 685,\n",
       " 'reminders': 686,\n",
       " 'plowed': 687,\n",
       " 'schedulers': 688,\n",
       " 'overmuch': 689,\n",
       " 'fratelli': 690,\n",
       " 'reportage': 691,\n",
       " 'cardinal': 692,\n",
       " 'atoned': 693,\n",
       " 'times': 694,\n",
       " 'oversimply': 695,\n",
       " 'jobs': 696,\n",
       " 'reichstagsbuilding': 697,\n",
       " 'hails': 698,\n",
       " 'final': 699,\n",
       " 'cheree': 700,\n",
       " 'subtract': 701,\n",
       " 'logging': 702,\n",
       " 'annexed': 703,\n",
       " 'sssss': 704,\n",
       " 'ground': 705,\n",
       " 'bowdlerized': 706,\n",
       " 'titledunsolved': 707,\n",
       " 'orin': 708,\n",
       " 'wiarton': 709,\n",
       " 'afforded': 710,\n",
       " 'skeptic': 711,\n",
       " 'weaponry': 712,\n",
       " 'immigrant': 713,\n",
       " 'filmatography': 714,\n",
       " 'bazaar': 715,\n",
       " 'lindy': 716,\n",
       " 'abkani': 717,\n",
       " 'mccree': 718,\n",
       " 'redubbed': 719,\n",
       " 'cgi': 720,\n",
       " 'dakota': 721,\n",
       " 'termed': 722,\n",
       " 'janowski': 723,\n",
       " 'espoir': 724,\n",
       " 'sinthome': 725,\n",
       " 'woche': 726,\n",
       " 'gariazzo': 727,\n",
       " 'ludicrious': 728,\n",
       " 'saleswomen': 729,\n",
       " 'annabeth': 730,\n",
       " 'usenet': 731,\n",
       " 'immediacy': 732,\n",
       " 'backlashes': 733,\n",
       " 'sarasohn': 734,\n",
       " 'explainable': 735,\n",
       " 'suburban': 736,\n",
       " 'drab': 737,\n",
       " 'changin': 738,\n",
       " 'finese': 739,\n",
       " 'boring': 740,\n",
       " 'manlis': 741,\n",
       " 'bgr': 742,\n",
       " 'dozes': 743,\n",
       " 'punkah': 744,\n",
       " 'tweedy': 745,\n",
       " 'chandulal': 746,\n",
       " 'beholder': 747,\n",
       " 'dkd': 748,\n",
       " 'anethesia': 749,\n",
       " 'classically': 750,\n",
       " 'levelled': 751,\n",
       " 'twentysomething': 752,\n",
       " 'rosentrasse': 753,\n",
       " 'doody': 754,\n",
       " 'badat': 755,\n",
       " 'jackhammers': 756,\n",
       " 'corny': 757,\n",
       " 'obsess': 758,\n",
       " 'lothar': 759,\n",
       " 'gymnastics': 760,\n",
       " 'utilitarian': 761,\n",
       " 'connors': 762,\n",
       " 'lameass': 763,\n",
       " 'guadalcanal': 764,\n",
       " 'scenic': 765,\n",
       " 'existed': 766,\n",
       " 'hasslehoff': 767,\n",
       " 'regan': 768,\n",
       " 'aborted': 769,\n",
       " 'moody': 770,\n",
       " 'stallonethat': 771,\n",
       " 'avantegardistic': 772,\n",
       " 'dont': 773,\n",
       " 'chatterboxes': 774,\n",
       " 'rosetto': 775,\n",
       " 'leer': 776,\n",
       " 'marionette': 777,\n",
       " 'monolithic': 778,\n",
       " 'firefly': 779,\n",
       " 'humphrey': 780,\n",
       " 'salaried': 781,\n",
       " 'fiedler': 782,\n",
       " 'farnsworth': 783,\n",
       " 'wyler': 784,\n",
       " 'pew': 785,\n",
       " 'heartened': 786,\n",
       " 'ruinous': 787,\n",
       " 'perth': 788,\n",
       " 'malibu': 789,\n",
       " 'ghostwriting': 790,\n",
       " 'sichuan': 791,\n",
       " 'noe': 792,\n",
       " 'boneheaded': 793,\n",
       " 'knowns': 794,\n",
       " 'eddie': 795,\n",
       " 'denmark': 796,\n",
       " 'thirtyish': 797,\n",
       " 'omnibusan': 798,\n",
       " 'musketeers': 799,\n",
       " 'actions': 800,\n",
       " 'raskolnikov': 801,\n",
       " 'smalltime': 802,\n",
       " 'crowne': 803,\n",
       " 'mel': 804,\n",
       " 'flier': 805,\n",
       " 'athletes': 806,\n",
       " 'yale': 807,\n",
       " 'persistance': 808,\n",
       " 'fkers': 809,\n",
       " 'germanish': 810,\n",
       " 'brang': 811,\n",
       " 'lisbeth': 812,\n",
       " 'empathy': 813,\n",
       " 'prevails': 814,\n",
       " 'yamadera': 815,\n",
       " 'auctions': 816,\n",
       " 'deviating': 817,\n",
       " 'aicn': 818,\n",
       " 'paternity': 819,\n",
       " 'gearing': 820,\n",
       " 'johnsons': 821,\n",
       " 'sobre': 822,\n",
       " 'membury': 823,\n",
       " 'bookish': 824,\n",
       " 'inspecting': 825,\n",
       " 'fit': 826,\n",
       " 'euphemisms': 827,\n",
       " 'contempory': 828,\n",
       " 'refined': 829,\n",
       " 'bodybuilders': 830,\n",
       " 'posers': 831,\n",
       " 'moloney': 832,\n",
       " 'pilling': 833,\n",
       " 'fictionalising': 834,\n",
       " 'arliss': 835,\n",
       " 'haven': 836,\n",
       " 'z': 837,\n",
       " 'melchior': 838,\n",
       " 'retains': 839,\n",
       " 'segregating': 840,\n",
       " 'coptic': 841,\n",
       " 'bute': 842,\n",
       " 'hostesses': 843,\n",
       " 'deducing': 844,\n",
       " 'realist': 845,\n",
       " 'depicted': 846,\n",
       " 'egger': 847,\n",
       " 'pals': 848,\n",
       " 'cazal': 849,\n",
       " 'bamboo': 850,\n",
       " 'tilted': 851,\n",
       " 'idiosyncrasy': 852,\n",
       " 'tersteeghe': 853,\n",
       " 'tunes': 854,\n",
       " 'iggy': 855,\n",
       " 'escorting': 856,\n",
       " 'absorbed': 857,\n",
       " 'homoeroticism': 858,\n",
       " 'illiterate': 859,\n",
       " 'jumpstart': 860,\n",
       " 'zhang': 861,\n",
       " 'carrying': 862,\n",
       " 'childhoods': 863,\n",
       " 'stillness': 864,\n",
       " 'lippman': 865,\n",
       " 'midsts': 866,\n",
       " 'dornwinkle': 867,\n",
       " 'kliegs': 868,\n",
       " 'beavis': 869,\n",
       " 'mathematicians': 870,\n",
       " 'devoreaux': 871,\n",
       " 'kutchek': 872,\n",
       " 'grotesquesat': 873,\n",
       " 'bamboozling': 874,\n",
       " 'unimaginative': 875,\n",
       " 'raymond': 876,\n",
       " 'tyron': 877,\n",
       " 'chilean': 878,\n",
       " 'rhind': 879,\n",
       " 'seftel': 880,\n",
       " 'thespic': 881,\n",
       " 'etches': 882,\n",
       " 'hangs': 883,\n",
       " 'bergdof': 884,\n",
       " 'tire': 885,\n",
       " 'toady': 886,\n",
       " 'soured': 887,\n",
       " 'jox': 888,\n",
       " 'thesigner': 889,\n",
       " 'mie': 890,\n",
       " 'mishap': 891,\n",
       " 'declarative': 892,\n",
       " 'coincidence': 893,\n",
       " 'btchiness': 894,\n",
       " 'blessing': 895,\n",
       " 'aquires': 896,\n",
       " 'malcolm': 897,\n",
       " 'hailsham': 898,\n",
       " 'chop': 899,\n",
       " 'starters': 900,\n",
       " 'stats': 901,\n",
       " 'dereliction': 902,\n",
       " 'priding': 903,\n",
       " 'kumble': 904,\n",
       " 'nook': 905,\n",
       " 'show': 906,\n",
       " 'crystal': 907,\n",
       " 'transaction': 908,\n",
       " 'dweller': 909,\n",
       " 'panty': 910,\n",
       " 'bredell': 911,\n",
       " 'shiploads': 912,\n",
       " 'piznarski': 913,\n",
       " 'synthesizers': 914,\n",
       " 'incomprehensibility': 915,\n",
       " 'blissfully': 916,\n",
       " 'musicthe': 917,\n",
       " 'temporary': 918,\n",
       " 'nack': 919,\n",
       " 'grinchy': 920,\n",
       " 'twyker': 921,\n",
       " 'widening': 922,\n",
       " 'shakewspeare': 923,\n",
       " 'longhetti': 924,\n",
       " 'barred': 925,\n",
       " 'storytelling': 926,\n",
       " 'fixated': 927,\n",
       " 'davalos': 928,\n",
       " 'baseless': 929,\n",
       " 'frikkin': 930,\n",
       " 'baltimorean': 931,\n",
       " 'synacures': 932,\n",
       " 'riots': 933,\n",
       " 'slagged': 934,\n",
       " 'knuckler': 935,\n",
       " 'swedlow': 936,\n",
       " 'defect': 937,\n",
       " 'bmoviefreak': 938,\n",
       " 'electrician': 939,\n",
       " 'cloying': 940,\n",
       " 'tersely': 941,\n",
       " 'jillson': 942,\n",
       " 'gamecube': 943,\n",
       " 'helms': 944,\n",
       " 'loa': 945,\n",
       " 'subpoints': 946,\n",
       " 'psychic': 947,\n",
       " 'uggh': 948,\n",
       " 'metaphoric': 949,\n",
       " 'buch': 950,\n",
       " 'consenting': 951,\n",
       " 'pdf': 952,\n",
       " 'whacking': 953,\n",
       " 'slicked': 954,\n",
       " 'idealizing': 955,\n",
       " 'gathers': 956,\n",
       " 'guilts': 957,\n",
       " 'carapace': 958,\n",
       " 'ashcroft': 959,\n",
       " 'rendering': 960,\n",
       " 'hoopers': 961,\n",
       " 'mikl': 962,\n",
       " 'paige': 963,\n",
       " 'guillotines': 964,\n",
       " 'pko': 965,\n",
       " 'blankfield': 966,\n",
       " 'epiphany': 967,\n",
       " 'ally': 968,\n",
       " 'uttered': 969,\n",
       " 'aldridge': 970,\n",
       " 'never': 971,\n",
       " 'shimomo': 972,\n",
       " 'zacatecas': 973,\n",
       " 'winked': 974,\n",
       " 'supblot': 975,\n",
       " 'phineas': 976,\n",
       " 'object': 977,\n",
       " 'lagomorpha': 978,\n",
       " 'crappily': 979,\n",
       " 'expansionist': 980,\n",
       " 'hdn': 981,\n",
       " 'fanatic': 982,\n",
       " 'arranger': 983,\n",
       " 'degeneration': 984,\n",
       " 'holographic': 985,\n",
       " 'pfieffer': 986,\n",
       " 'calibanian': 987,\n",
       " 'meander': 988,\n",
       " 'library': 989,\n",
       " 'skitz': 990,\n",
       " 'theby': 991,\n",
       " 'quintana': 992,\n",
       " 'spinozean': 993,\n",
       " 'macfadyen': 994,\n",
       " 'zombiesnatch': 995,\n",
       " 'twitter': 996,\n",
       " 'glassy': 997,\n",
       " 'became': 998,\n",
       " 'dusenberry': 999,\n",
       " 'motorised': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 标签 --> 1， 0 转换\n",
    "# positive : 1,  negative : 0\n",
    "\n",
    "label_int = np.array([1 if x == 'positive' else 0 for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 12500, 0: 12501})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 清理文本太短以及过长的样本\n",
    "\n",
    "# 统计文本中，每条评论的长度\n",
    "sentence_length = [len(sentence.split()) for sentence in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(sentence_length) # 统计不同长度的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小评论长度\n",
    "min_sen = min(sorted(counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大评论长度\n",
    "max_sen = max(sorted(counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 min 和 max 对应的索引\n",
    "\n",
    "min_index = [i for i, length in enumerate(sentence_length) if length == min_sen[0]]\n",
    "\n",
    "max_index = [i for i, length in enumerate(sentence_length) if length == max_sen[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25000]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3908]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本数量：  25001\n",
      "新文本数量:  25000\n"
     ]
    }
   ],
   "source": [
    "# 根据索引删除文本中过短或过长的评论\n",
    "\n",
    "new_text = np.delete(clean_text, min_index)\n",
    "\n",
    "print(\"原始文本数量： \", len(clean_text))\n",
    "print(\"新文本数量: \", len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本数量：  25000\n",
      "新文本数量:  24999\n"
     ]
    }
   ],
   "source": [
    "new_text2 = np.delete(new_text, max_index)\n",
    "\n",
    "print(\"原始文本数量： \", len(new_text))\n",
    "print(\"新文本数量: \", len(new_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始标签数量：  25001\n",
      "新标签数量：  24999\n"
     ]
    }
   ],
   "source": [
    "# 同样需要在标签集中根据索引删除对应的标签\n",
    "\n",
    "new_labels = np.delete(label_int, min_index)\n",
    "\n",
    "new_labels = np.delete(new_labels, max_index)\n",
    "\n",
    "print(\"原始标签数量： \", len(label_int))\n",
    "print(\"新标签数量： \", len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 将单词映射为整型\n",
    "\n",
    "text_ints = []\n",
    "for sentence in new_text2:\n",
    "    sample = list()\n",
    "    for word in sentence.split():\n",
    "        int_value = word_int[word] # 获取到单词对应的键\n",
    "        sample.append(int_value)\n",
    "    text_ints.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64942, 43083, 34620, 34779, 21352, 2327, 16879, 20939, 6607, 41529, 56330, 65539, 19938, 43495, 56110, 64236, 43265, 7881, 59568, 70628, 19938, 53981, 7248, 14558, 35470, 41529, 57534, 38880, 42828, 4726, 13691, 66350, 51012, 64942, 43083, 11443, 73720, 34620, 12184, 72774, 13691, 42912, 55385, 34620, 53981, 41529, 35700, 13691, 51370, 15418, 41529, 8257, 1954, 8851, 67226, 43547, 36027, 33502, 10042, 9113, 53981, 47286, 41529, 29895, 27799, 41529, 61137, 61743, 70013, 33526, 4726, 27799, 41529, 15674, 8425, 27985, 14511, 10042, 1954, 56396, 8425, 42225, 41529, 3436, 35470, 64922, 34779, 42125, 35102, 35035, 13691, 7893, 58966, 41529, 7881, 8425, 3046, 31693, 6607, 43083, 34779, 57371, 43438, 34442, 8425, 46527, 4545, 13691, 30752, 42377, 27799, 44460, 53981, 42125, 54373, 13691, 64942, 43083, 8425, 37740, 51012, 35875, 41935, 27799, 7248, 43491, 33161, 51012, 64942, 43083, 34620, 64870, 36713, 66264, 34779, 20968, 51012, 16879, 69751, 65440]\n"
     ]
    }
   ],
   "source": [
    "print(text_ints[0]) # 第一条评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24999"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_ints) # 总的评论数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 设定统一的文本长度，对整个文本数据中的每条评论进行填充或截断\n",
    "# 设定每条评论固定长度为200个单词，不足的评论用0填充，超过的直接截断\n",
    "\n",
    "def reset_text(text, seq_len):\n",
    "    dataset = np.zeros((len(text), seq_len))\n",
    "    for index, sentence in enumerate(text):\n",
    "        if len(sentence) < seq_len:\n",
    "            dataset[index, :len(sentence)] = sentence\n",
    "        else:\n",
    "            dataset[index, :] = sentence[:seq_len] # 截断\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reset_text(text_ints, seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24999, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64942. 43083. 34620. 34779. 21352.  2327. 16879. 20939.  6607. 41529.\n",
      " 56330. 65539. 19938. 43495. 56110. 64236. 43265.  7881. 59568. 70628.\n",
      " 19938. 53981.  7248. 14558. 35470. 41529. 57534. 38880. 42828.  4726.\n",
      " 13691. 66350. 51012. 64942. 43083. 11443. 73720. 34620. 12184. 72774.\n",
      " 13691. 42912. 55385. 34620. 53981. 41529. 35700. 13691. 51370. 15418.\n",
      " 41529.  8257.  1954.  8851. 67226. 43547. 36027. 33502. 10042.  9113.\n",
      " 53981. 47286. 41529. 29895. 27799. 41529. 61137. 61743. 70013. 33526.\n",
      "  4726. 27799. 41529. 15674.  8425. 27985. 14511. 10042.  1954. 56396.\n",
      "  8425. 42225. 41529.  3436. 35470. 64922. 34779. 42125. 35102. 35035.\n",
      " 13691.  7893. 58966. 41529.  7881.  8425.  3046. 31693.  6607. 43083.\n",
      " 34779. 57371. 43438. 34442.  8425. 46527.  4545. 13691. 30752. 42377.\n",
      " 27799. 44460. 53981. 42125. 54373. 13691. 64942. 43083.  8425. 37740.\n",
      " 51012. 35875. 41935. 27799.  7248. 43491. 33161. 51012. 64942. 43083.\n",
      " 34620. 64870. 36713. 66264. 34779. 20968. 51012. 16879. 69751. 65440.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3.1 数据类型转换\n",
    "dataset_tensor = torch.from_numpy(dataset)\n",
    "label_tensor = torch.from_numpy(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24999, 200])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24999])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数： 24999\n",
      "训练样本数： 19999\n",
      "验证样本数： 2500\n",
      "测试样本数： 2500\n"
     ]
    }
   ],
   "source": [
    "# 3.2 数据分割，train, val, test\n",
    "\n",
    "# 总样本数\n",
    "all_samples = len(dataset_tensor)\n",
    "print(\"总样本数：\",all_samples)\n",
    "\n",
    "# 设置比例\n",
    "ratio = 0.8\n",
    "train_size = int(all_samples * 0.8) # 训练样本数\n",
    "print(\"训练样本数：\",train_size)\n",
    "\n",
    "rest_size = all_samples - train_size # 剩余样本数\n",
    "\n",
    "val_size = int(rest_size * 0.5) # 验证样本数\n",
    "print(\"验证样本数：\", val_size)\n",
    "\n",
    "test_size = int(rest_size * 0.5) # 测试样本数\n",
    "print(\"测试样本数：\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取train, val, test 样本\n",
    "\n",
    "# train\n",
    "train = dataset_tensor[:train_size]\n",
    "train_labels = label_tensor[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19999, 200])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19999])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剩余样本\n",
    "rest_samples = dataset_tensor[train_size:]\n",
    "rest_labels = label_tensor[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val = rest_samples[:val_size]\n",
    "val_labels = rest_labels[:val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500, 200])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test = rest_samples[val_size:]\n",
    "test_labels = rest_labels[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500, 200])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 通过DataLoader按批处理数据\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 对数据进行封装：(评论，标签)\n",
    "train_dataset = TensorDataset(train, train_labels)\n",
    "val_dataset = TensorDataset(val, val_labels)\n",
    "test_dataset = TensorDataset(test, test_labels)\n",
    "\n",
    "batch_size = 128\n",
    "# 批处理\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取train中的一批数据\n",
    "data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 200])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义网络模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout=0.5):\n",
    "        super(sentiment, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim) # 词嵌入层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        x shape : (batch_size, seq_len, features)\n",
    "\n",
    "        '''\n",
    "        batch_size = x.size(0) # 获取batch_size\n",
    "        x = x.long() # 类型转换\n",
    "        #print('x shape : ', x.shape) # torch.Size([128, 200])\n",
    "        embeds = self.embedding(x) # 词嵌入表示\n",
    "        #print('embeds shape : ', embeds.shape) # torch.Size([128, 200, 300])\n",
    "        out, hidden = self.lstm(embeds, hidden) # lstm out shape : (batch_size, seq_len, hidden_dim)\n",
    "        #print('out_1 shape : ', out.shape) # torch.Size([128, 200, 256])\n",
    "        #print('hidden_0 shape : ', hidden[0].shape) # torch.Size([2, 128, 256])\n",
    "        #print('hidden_1 shape : ', hidden[1].shape) # torch.Size([2, 128, 256])\n",
    "        out = out.reshape(-1, self.hidden_dim) # （batch_size * seq_len, hidden_dim）\n",
    "        #print('out_2 shape : ', out.shape) # torch.Size([25600, 256])\n",
    "        out = self.linear(out) # 全连接层\n",
    "        #print('out_3 shape : ', out.shape) # torch.Size([25600, 1])\n",
    "        sigmoid_out = self.sigmoid(out) #\n",
    "        #print('sigmoid_out_1 shape : ', sigmoid_out.shape) # torch.Size([25600, 1])\n",
    "        sigmoid_out = sigmoid_out.reshape(batch_size, -1)\n",
    "        #print('sigmoid_out_2 shape : ', sigmoid_out.shape) # torch.Size([128, 200])\n",
    "        sigmoid_out = sigmoid_out[:, -1] # 获取最后一批的标签\n",
    "        #print('sigmoid_out_3 shape : ', sigmoid_out.shape) # torch.Size([128])\n",
    "        return sigmoid_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #print(\"weghit :\", weight.shape) # torch.Size([74073, 300])\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化超参数\n",
    "input_size = len(word_int) + 1 # 输入（不同的单词个数）\n",
    "output_size = 1 # 输出\n",
    "embedding_dim = 400 # 词嵌入维度\n",
    "hidden_dim = 128 # 隐藏层节点个数\n",
    "num_layers = 2 # lstm的层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74073"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型\n",
    "model = sentiment(input_size, embedding_dim, hidden_dim, output_size, num_layers)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss() # 损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 优化器\n",
    "num_epochs = 50 # 循环次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练模型\n",
    "def train(model, device, data_loader, criterion, optimizer, num_epochs, val_loader):\n",
    "    history = list()\n",
    "    for epoch in range(num_epochs):\n",
    "        hs = model.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        train_correct = 0.0\n",
    "        model.train()\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device) # 部署到device\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad() # 梯度置零\n",
    "            output, hs = model(data, hs) # 模型训练\n",
    "            hs = tuple([h.data for h in hs])\n",
    "            #print('output shape : ', output.shape) # torch.Size([128])\n",
    "            loss = criterion(output, target.float()) # 计算损失\n",
    "            train_loss.append(loss.item()) # 累计损失\n",
    "            loss.backward() # 反向传播\n",
    "            optimizer.step() # 参数更新\n",
    "            train_correct += torch.sum(output==target) # 比较\n",
    "\n",
    "        # 模型验证\n",
    "        model.eval()\n",
    "        hs = model.init_hidden(batch_size)\n",
    "        val_loss = []\n",
    "        val_correct = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                preds, hs = model(data, hs) # 验证\n",
    "                hs = tuple([h.data for h in hs])\n",
    "                loss = criterion(preds, target.float()) # 计算损失\n",
    "                val_loss.append(loss.item()) # 累计损失\n",
    "                val_correct += torch.sum(preds==target) # 比较\n",
    "#             history['val_loss'].append(np.mean(val_loss))\n",
    "#             history['val_correct'].append(np.mean(val_correct))\n",
    "#         history['train_loss'].append(np.mean(train_loss))\n",
    "#         history['train_correct'].append(np.mean(train_correct))\n",
    "        print(f'Epoch {epoch}/{num_epochs} --- train loss {np.round(np.mean(train_loss), 5)} --- val loss {np.round(np.mean(val_loss),5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Pychram_project\\PyTorch\\Tommy\\13_PyTorch LSTM 情感分类【下篇】\\PyTorch LSTM 情感分类【下篇】.ipynb Cell 83\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, device, train_loader, criterion, optimizer, num_epochs, val_loader)\n",
      "\u001b[1;32me:\\Pychram_project\\PyTorch\\Tommy\\13_PyTorch LSTM 情感分类【下篇】\\PyTorch LSTM 情感分类【下篇】.ipynb Cell 83\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, data_loader, criterion, optimizer, num_epochs, val_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target\u001b[39m.\u001b[39mfloat()) \u001b[39m# 计算损失\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem()) \u001b[39m# 累计损失\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# 反向传播\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# 参数更新\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Pychram_project/PyTorch/Tommy/13_PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91/PyTorch%20LSTM%20%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E3%80%90%E4%B8%8B%E7%AF%87%E3%80%91.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(output\u001b[39m==\u001b[39mtarget) \u001b[39m# 比较\u001b[39;00m\n",
      "File \u001b[1;32md:\\SoftWare\\Anaconda\\envs\\DL\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\SoftWare\\Anaconda\\envs\\DL\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, device, train_loader, criterion, optimizer, num_epochs, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "def test(model, data_loader, device, criterion):\n",
    "    test_losses = []\n",
    "    num_correct = 0\n",
    "    # 初始化隐藏状态\n",
    "    hs = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    for i, dataset in enumerate(data_loader):\n",
    "        data = dataset[0].to(device) # 部署到device\n",
    "        target = dataset[1].to(device)\n",
    "        output, hs = model(data, hs) # 测试\n",
    "        loss = criterion(output, target.float()) # 计算损失\n",
    "        pred = torch.round(output) # 将预测值进行四舍五入，转换为0 或 1\n",
    "        test_losses.append(loss.item()) # 保存损失\n",
    "        correct_tensor = pred.eq(target.float().view_as(pred)) # 返回一堆True 或 False\n",
    "        correct = correct_tensor.cpu().numpy()\n",
    "        result = np.sum(correct)\n",
    "        num_correct += result\n",
    "        #print(\"num correct : \", num_correct)\n",
    "        print(f'Batch {i}')\n",
    "        print(f'loss : {np.round(np.mean(loss.item()), 3)}')\n",
    "        print(f'accuracy : {np.round(result / len(data), 3) * 100} %')\n",
    "        print()\n",
    "    print(\"总的测试损失 test loss : {:.2f}\".format(np.mean(test_losses)))\n",
    "    print(\"总的测试准确率 test accuracy : {:.2f}\".format(np.mean(num_correct / len(data_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "loss : 1.749\n",
      "accuracy : 68.0 %\n",
      "\n",
      "Batch 1\n",
      "loss : 1.57\n",
      "accuracy : 71.1 %\n",
      "\n",
      "Batch 2\n",
      "loss : 1.489\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 3\n",
      "loss : 1.456\n",
      "accuracy : 75.0 %\n",
      "\n",
      "Batch 4\n",
      "loss : 1.201\n",
      "accuracy : 77.3 %\n",
      "\n",
      "Batch 5\n",
      "loss : 1.432\n",
      "accuracy : 73.4 %\n",
      "\n",
      "Batch 6\n",
      "loss : 1.409\n",
      "accuracy : 74.2 %\n",
      "\n",
      "Batch 7\n",
      "loss : 1.507\n",
      "accuracy : 71.1 %\n",
      "\n",
      "Batch 8\n",
      "loss : 1.626\n",
      "accuracy : 70.3 %\n",
      "\n",
      "Batch 9\n",
      "loss : 1.331\n",
      "accuracy : 77.3 %\n",
      "\n",
      "Batch 10\n",
      "loss : 1.654\n",
      "accuracy : 71.89999999999999 %\n",
      "\n",
      "Batch 11\n",
      "loss : 1.355\n",
      "accuracy : 75.0 %\n",
      "\n",
      "Batch 12\n",
      "loss : 1.45\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 13\n",
      "loss : 1.737\n",
      "accuracy : 67.2 %\n",
      "\n",
      "Batch 14\n",
      "loss : 1.577\n",
      "accuracy : 72.7 %\n",
      "\n",
      "Batch 15\n",
      "loss : 2.027\n",
      "accuracy : 65.60000000000001 %\n",
      "\n",
      "Batch 16\n",
      "loss : 1.747\n",
      "accuracy : 67.2 %\n",
      "\n",
      "Batch 17\n",
      "loss : 1.482\n",
      "accuracy : 75.8 %\n",
      "\n",
      "Batch 18\n",
      "loss : 1.498\n",
      "accuracy : 69.5 %\n",
      "\n",
      "总的测试损失 test loss : 1.54\n",
      "总的测试准确率 test accuracy : 0.70\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, device, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测（测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例1\n",
    "text = 'this movie is so amazing. the plot is attractive. and I really like it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步：文本转索引（整数）\n",
    "from string import punctuation\n",
    "\n",
    "def converts(text):\n",
    "    # 去除标点符号\n",
    "    new_text = ''.join([char for char in text if char not in punctuation])\n",
    "    print(\"new text :\\n\", new_text)\n",
    "    # 文本映射为索引\n",
    "    text_ints = [word_int[word.lower()] for word in new_text.split()]\n",
    "    print(\"文本映射为索引：\\n\", text_ints)\n",
    "    return text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new text :\n",
      " this movie is so amazing the plot is attractive and I really like it\n",
      "文本映射为索引：\n",
      " [12542, 26331, 68363, 55476, 13980, 4157, 3491, 68363, 25499, 11623, 18926, 8585, 69980, 72091]\n"
     ]
    }
   ],
   "source": [
    "text_ints = converts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12542,\n",
       " 26331,\n",
       " 68363,\n",
       " 55476,\n",
       " 13980,\n",
       " 4157,\n",
       " 3491,\n",
       " 68363,\n",
       " 25499,\n",
       " 11623,\n",
       " 18926,\n",
       " 8585,\n",
       " 69980,\n",
       " 72091]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本对齐，sequence_length = 200\n",
    "new_text_ints = reset_text([text_ints], seq_len=200) # 注意这里要添加一个[]，因为，reset_text处理的二维数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12542., 26331., 68363., 55476., 13980.,  4157.,  3491., 68363.,\n",
       "        25499., 11623., 18926.,  8585., 69980., 72091.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_ints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# numpy --> tensor\n",
    "text_tensor = torch.from_numpy(new_text_ints)\n",
    "\n",
    "print(text_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "def predict(model, text_tensor, device):\n",
    "    batch_size = text_tensor.size(0) # 这里是1\n",
    "    hs = model.init_hidden(batch_size) # 初始化隐藏状态\n",
    "    text_tensor = text_tensor.to(device)\n",
    "    pred, hs = model(text_tensor, hs) # 判断\n",
    "    print(\"概率值：\", pred.item())\n",
    "    # 将pred概率值转换为0或1\n",
    "    pred = torch.round(pred)\n",
    "    print(\"类别值：\", pred.item())\n",
    "    # 判断\n",
    "    if pred.data == 1:\n",
    "        print(\"评论正面\")\n",
    "    else:\n",
    "        print(\"评论反面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "概率值： 0.839598536491394\n",
      "类别值： 1.0\n",
      "评论正面\n"
     ]
    }
   ],
   "source": [
    "predict(model, text_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "61b55140bc9bd5084f21fbaa66b1edd89209bd8dc911d8d1825c542d6d54ebb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
